

**Chad Dorsey's Personal Meeting Room**
=======================================

Time: 2025\-08\-27T13:48:22Z

Transcript:WEBVTT

1

00:00:02\.280 \-\-\> 00:00:03\.170

Leslie Bondaryk (she/her): Right.

2

00:00:03\.840 \-\-\> 00:00:06\.860

Leslie Bondaryk (she/her): I was still very much…

3

00:00:07\.500 \-\-\> 00:00:16\.430

Leslie Bondaryk (she/her): you know, very much focused on the rubric stuff that we have been doing for mods and for CMP.

4

00:00:16\.830 \-\-\> 00:00:27\.249

Leslie Bondaryk (she/her): Because I was, and remain convinced that you can't just say, go ahead, evaluate this thingy.

5

00:00:27\.410 \-\-\> 00:00:33\.360

Leslie Bondaryk (she/her): And tell me what you think about it, and expect to get anything that's pedagogically useful, right?

6

00:00:33\.360 \-\-\> 00:00:34\.270

Chad Dorsey: Yep.

7

00:00:34\.270 \-\-\> 00:00:45\.629

Leslie Bondaryk (she/her): There's a lot of people who are trying to score things, and that's kind of okay, and we may wind up in the project with Ito having to do some of that.

8

00:00:45\.950 \-\-\> 00:00:46\.930

Chad Dorsey: Right? Like, say.

9

00:00:46\.930 \-\-\> 00:00:53\.719

Leslie Bondaryk (she/her): yes, this program is right, or no, here are the things you need to change in order to make it right, that kind of thing.

10

00:00:54\.290 \-\-\> 00:00:55\.080

Leslie Bondaryk (she/her): …

11

00:00:55\.810 \-\-\> 00:01:12\.970

Leslie Bondaryk (she/her): But I'm not so interested in that, partly because lots of people are tackling that problem, and partly because I don't think it's the interesting thing to do with AI, right? But I have been very focused on, you know, can we… can we make an AI system

12

00:01:13\.270 \-\-\> 00:01:27\.420

Leslie Bondaryk (she/her): evaluate around a framework, right? And does that evaluation that it does match what a human would say? And for the mods project, they've been paying for most of that development effort.

13

00:01:27\.420 \-\-\> 00:01:33\.439

Leslie Bondaryk (she/her): But they don't care, right? The honest truth is that it really doesn't matter to them much whether…

14

00:01:33\.490 \-\-\> 00:01:39\.910

Leslie Bondaryk (she/her): We get that evaluation right, because as long as it stimulates conversation about the design, it doesn't matter.

15

00:01:40\.040 \-\-\> 00:01:49\.300

Leslie Bondaryk (she/her): The math… the math people have been spending a lot of time doing human tagging, and I have built facilities right into the product

16

00:01:49\.300 \-\-\> 00:02:01\.140

Leslie Bondaryk (she/her): to let them do that tagging, because I think it's very important for us to have all this data in the same format, in the same system, right? You know, it's got to look the same, whether it's human or an AI, and we need

17

00:02:01\.140 \-\-\> 00:02:04\.389

Leslie Bondaryk (she/her): We need to be able to, you know, almost blur that distinction.

18

00:02:05\.660 \-\-\> 00:02:14\.000

Leslie Bondaryk (she/her): The thing that's come up recently, and I think this is driven by some of the recent improvements in LLMs.

19

00:02:14\.000 \-\-\> 00:02:24\.579

Leslie Bondaryk (she/her): is that what everybody wants is not so much on the student evaluation end, but more on the, how am I posing

20

00:02:24\.580 \-\-\> 00:02:36\.439

Leslie Bondaryk (she/her): the problem end, right? It's the contextualization, customization of the ask to the student, of the scaffolding for the student, right? What do you ask them to do next?

21

00:02:37\.680 \-\-\> 00:02:42\.680

Leslie Bondaryk (she/her): in line with the thing that we were asking Google to fund us to do.

22

00:02:43\.630 \-\-\> 00:02:51\.580

Leslie Bondaryk (she/her): … And I think… I think with enough data, We can get there.

23

00:02:52\.720 \-\-\> 00:02:54\.709

Leslie Bondaryk (she/her): The… the problem…

24

00:02:55\.020 \-\-\> 00:03:01\.069

Leslie Bondaryk (she/her): the problem for us seems to be, and it's… this is actually not that different from the Debye

25

00:03:01\.280 \-\-\> 00:03:07\.650

Leslie Bondaryk (she/her): thing, in my opinion, right? The problem is that you need a lot of backdrop for that.

26

00:03:08\.010 \-\-\> 00:03:19\.140

Leslie Bondaryk (she/her): And… if you are building a chatbot based on your, you know, corporate knowledge base, you do RAG training.

27

00:03:19\.320 \-\-\> 00:03:26\.869

Leslie Bondaryk (she/her): Ours is weirder than that because it's dynamic, right? It depends on what gets said by the student, what gets said by the class.

28

00:03:27\.030 \-\-\> 00:03:40\.100

Leslie Bondaryk (she/her): Right? And my take so far is that students won't generate on their own enough information, even if it's a whole… you know, let's suppose it's a whole year class, right? Even after.

29

00:03:40\.100 \-\-\> 00:03:40\.620

Chad Dorsey: Right.

30

00:03:40\.830 \-\-\> 00:03:47\.259

Leslie Bondaryk (she/her): Of their typing, like, the minimal kinds of things that students tend to type into a system.

31

00:03:47\.260 \-\-\> 00:03:49\.860

Chad Dorsey: We've got 30 worksheets, essentially, yeah.

32

00:03:49\.860 \-\-\> 00:04:08\.859

Leslie Bondaryk (she/her): Right, it's like, there's not a lot there to go on, so my hope is that at least by opening this up as far as the whole class is concerned, and maybe, like, giving the teachers stuff extra weight, so if they want to prep a class, they can type in, like, some extra stuff and have it skew things.

33

00:04:09\.030 \-\-\> 00:04:25\.409

Leslie Bondaryk (she/her): you can create enough of a raggish kind of database that you can then use that to, like, guide what goes forward there. So, lately, we have been trying sort of different experiments to get that

34

00:04:25\.480 \-\-\> 00:04:49\.969

Leslie Bondaryk (she/her): done, and whether it is skewing based on personal preferences for the student, or skewing based on the fact that the student is doing data analysis in the terms of… in terms of Dubai, and so you should give them answers that are, you know, that sort of recognize the kind of activity that they are doing, right? Like, we need to… we need to get that kind of training in.

35

00:04:50\.180 \-\-\> 00:05:00\.660

Leslie Bondaryk (she/her): And we need to build in infrastructures that allow us to regenerate that context on the fly. And that's been our biggest technical problem

36

00:05:00\.660 \-\-\> 00:05:15\.800

Leslie Bondaryk (she/her): at least from my perspective of the moment, is that all of these systems that are out there to let you do that, right, to put a RAG database together, you know, whatever, are either kind of slow to reconstitute themselves.

37

00:05:15\.800 \-\-\> 00:05:33\.809

Leslie Bondaryk (she/her): or don't remember enough, right? And so Ethan is now building this whole system to, you know, kind of rebootstrap the thing on the fly, given the weird sporadic nature of when people use our stuff, right? They're not using it all day long, sometimes they don't use it.

38

00:05:33\.810 \-\-\> 00:05:34\.419

Chad Dorsey: And for one.

39

00:05:34\.640 \-\-\> 00:05:50\.299

Leslie Bondaryk (she/her): And then suddenly there we are again, right? So there's this problem where we have to kind of, like, rebuild all of our background knowledge. And we're trying a couple different things now, but it takes time, and it takes work, and it takes funding, and….

40

00:05:50\.300 \-\-\> 00:05:52\.050

Chad Dorsey: Right, and that, …

41

00:05:52\.640 \-\-\> 00:06:02\.310

Chad Dorsey: I'm just thinking and working on side projects for my… with my own personal data on this, too, and just starting to get up to the right side of things, is that,

42

00:06:02\.850 \-\-\> 00:06:09\.569

Chad Dorsey: Is that… is that the embedding issue that you, like, I'm just… you're just making me recognize?

43

00:06:09\.640 \-\-\> 00:06:12\.160

Scott Cytacki: You have to… is it essentially….

44

00:06:12\.790 \-\-\> 00:06:31\.749

Chad Dorsey: retraining, and you have to re\-embed the whole database as a chunk because you're developing the relationships? Or is there, on the fly, sort of vectorizing, or is that not relevant because you're really just vectorizing against this sort of schema? You know, what's the… how does that RAG database get built up, and how…

45

00:06:31\.900 \-\-\> 00:06:38\.490

Chad Dorsey: How dependent is it upon, you know, some holistic process versus, you know, the… what's the on\-the\-fly problem?

46

00:06:40\.610 \-\-\> 00:06:42\.210

Chad Dorsey: If that makes any sense.

47

00:06:43\.120 \-\-\> 00:06:51\.499

Scott Cytacki: I mean, I think there's two parts. The part about remembering the conversation is kind of just separate, that's sort of just state saving, essentially, that's not easily.

48

00:06:51\.500 \-\-\> 00:06:54\.729

Chad Dorsey: That's all memory stuff that people are working on separately.

49

00:06:54\.730 \-\-\> 00:07:06\.800

Scott Cytacki: Yeah, but then the part about providing context based on the class or something else, then that is about storing the classes at work along with the teacher's work in a rag, and…

50

00:07:07\.200 \-\-\> 00:07:16\.080

Scott Cytacki: And I… yeah, I'm not familiar with the issues there that Leslie was talking about, but I… certainly, they're not designed to be fast, …

51

00:07:16\.780 \-\-\> 00:07:21\.650

Scott Cytacki: So, I can certainly see how it… if we're constantly changing the rag, it would get slow.

52

00:07:22\.180 \-\-\> 00:07:24\.420

Leslie Bondaryk (she/her): It won't… it won't be instant.

53

00:07:24\.560 \-\-\> 00:07:26\.140

Scott Cytacki: to… 2\.

54

00:07:26\.530 \-\-\> 00:07:29\.240

Scott Cytacki: Take into account something you just added to it.

55

00:07:29\.830 \-\-\> 00:07:31\.969

Scott Cytacki: But I don't think it's, like, that slow.

56

00:07:32\.310 \-\-\> 00:07:39\.029

Scott Cytacki: I would… I mean, my guess for most of these things, our biggest problem is going to be cost more than anything, like, trying to figure out how to optimize

57

00:07:39\.720 \-\-\> 00:07:44\.079

Scott Cytacki: This stuff so that it doesn't… Become too prohibitively expensive.

58

00:07:45\.090 \-\-\> 00:07:50\.109

Scott Cytacki: But we haven't really hit there yet, because we're so far on small numbers, so we haven't had to optimize that part, but….

59

00:07:52\.830 \-\-\> 00:08:06\.939

Leslie Bondaryk (she/her): You know, at the moment, we're kind of coming up with a couple of different strategies to basically do our analyses offline, you know, decide which analyses have to be fresh versus which ones can be, like, nightly.

60

00:08:07\.950 \-\-\> 00:08:11\.690

Leslie Bondaryk (she/her): You know, when can we offload stuff so that…

61

00:08:11\.800 \-\-\> 00:08:21\.060

Leslie Bondaryk (she/her): We've gone and crunched every document in a 25\-kid class, and their teachers work, you know, over the past month and a half.

62

00:08:21\.350 \-\-\> 00:08:31\.200

Leslie Bondaryk (she/her): Stash it, and then go figure out what we want to say about it, or summarize it somehow, you know, and then use that short thing

63

00:08:31\.760 \-\-\> 00:08:36\.940

Leslie Bondaryk (she/her): to drive… to drive the live interactive stuff that goes into that. Right.

64

00:08:37\.840 \-\-\> 00:08:45\.810

Leslie Bondaryk (she/her): So that's… I mean, like, that's the kind of stuff that I've been thinking about lately. I haven't run too much more ahead of that.

65

00:08:46\.010 \-\-\> 00:08:57\.970

Leslie Bondaryk (she/her): maybe Scott has, but I will say that there's opportunity that I haven't really even begun to think about in terms of contextualization and customization

66

00:08:58\.650 \-\-\> 00:09:00\.460

Leslie Bondaryk (she/her): for a teacher.

67

00:09:01\.020 \-\-\> 00:09:07\.330

Leslie Bondaryk (she/her): based on whatever parameters they feel are important, right? I am teaching in New England, I am.

68

00:09:07\.330 \-\-\> 00:09:07\.800

Chad Dorsey: changing.

69

00:09:07\.800 \-\-\> 00:09:26\.059

Leslie Bondaryk (she/her): in Southern California, we get earthquakes. I am teaching in, you know, Louisiana, and we get floods, right? That kind of thing, with the, you know, tsunami of OpenSci Ed requests that you've been getting, right? Like, I've been rolling that around in the back of my head.

70

00:09:26\.160 \-\-\> 00:09:36\.159

Leslie Bondaryk (she/her): But I don't know… I don't know how applicable the stuff I am working on right now within Clue, and, you know, to a tiny little extent within Kodak.

71

00:09:36\.470 \-\-\> 00:09:41\.490

Leslie Bondaryk (she/her): will make that possible, right? But that one seems…

72

00:09:41\.560 \-\-\> 00:09:56\.959

Leslie Bondaryk (she/her): almost more tractable to me. Like, I think I heard you say at some point, Scott, maybe we should have just an extra box on the STEM resource finder that says, you know, here's my context, here's what Open SciEd unit I'm using, can you recommend some…

73

00:09:57\.200 \-\-\> 00:09:58\.040

Leslie Bondaryk (she/her): Right.

74

00:09:58\.640 \-\-\> 00:10:09\.939

Leslie Bondaryk (she/her): and you recommend some sims, and we could go the next step with that and say, you know, can you actually generate me an activity, a version of the activity that you've written to go with these?

75

00:10:10\.430 \-\-\> 00:10:15\.090

Leslie Bondaryk (she/her): That is, you know, that is custom\-built, given what I just told you.

76

00:10:17\.850 \-\-\> 00:10:20\.910

Chad Dorsey: Yeah, and that latter piece, I think, starts to…

77

00:10:21\.000 \-\-\> 00:10:36\.130

Chad Dorsey: to verge onto the various levels of conversations and proposals that I think we've been tapping into, whether it's in Google or Island or what have you, on the customized simulation

78

00:10:36\.130 \-\-\> 00:10:49\.349

Chad Dorsey: side things, or the… somewhere in between there, customize page 3, 4, and 5 side of things, you know, or to the customize the whole activity side. What's the, …

79

00:10:49\.980 \-\-\> 00:11:05\.379

Chad Dorsey: what are the… and, you know, I've heard Hisun talk about, you know, use the word dynamic and activity player in the same sentence when she was thinking about Ireland, you know, not too long ago, and sort of recognizing the limitations and possibilities. So, what are the… what are the…

80

00:11:05\.520 \-\-\> 00:11:18\.890

Chad Dorsey: constraints and, and, you know, issues, or… and the sort of horizon dreams about customization in general, and sort of on\-the\-fly\-ish kinds of customization possibilities.

81

00:11:21\.260 \-\-\> 00:11:30\.029

Scott Cytacki: I mean, I would think it's… there's multiple, but one would be, from a research point of view, if everyone's getting a slightly different version of the activity, then that makes it really.

82

00:11:30\.030 \-\-\> 00:11:30\.399

Chad Dorsey: Oh, yeah.

83

00:11:30\.400 \-\-\> 00:11:32\.210

Scott Cytacki: to make conclusions.

84

00:11:32\.730 \-\-\> 00:11:35\.819

Scott Cytacki: Then there's the issue of, you know, making sure that it's…

85

00:11:36\.600 \-\-\> 00:11:50\.820

Scott Cytacki: valid in some sense, if we're gonna let it start to generate content. So, whatever, how we do the guardrails around it, will be interesting, but seem relatively doable.

86

00:11:51\.440 \-\-\> 00:11:52\.300

Scott Cytacki: ….

87

00:11:53\.250 \-\-\> 00:11:57\.589

Chad Dorsey: What are the technical blocks right now? I mean, there's…

88

00:11:57\.770 \-\-\> 00:12:05\.740

Chad Dorsey: making 3 versions of a activity… an activity player for 3 students in a class is… wouldn't happen by Friday.

89

00:12:06\.670 \-\-\> 00:12:18\.350

Scott Cytacki: I guess I'm… I was thinking more, like, not per class, but per class instead of per student. So, like, if we're giving it context of where your school is located, then that would just be a class\-wide customization.

90

00:12:18\.450 \-\-\> 00:12:22\.379

Scott Cytacki: And… but even with that, there's new issues of, well, yeah, how do…

91

00:12:23\.180 \-\-\> 00:12:31\.750

Scott Cytacki: the portal would have to have some changes or something, because we… I don't think we could just start storing every customized version in there, maybe?

92

00:12:31\.950 \-\-\> 00:12:43\.930

Scott Cytacki: I mean, it's actually kind of similar to what we ran into when we were trying to support ITSI, where we were allowing teachers to author, because they were basically making these kind of changes, where they were trying to contextualize the activities in minor ways.

93

00:12:43\.980 \-\-\> 00:12:48\.929

Chad Dorsey: But that meant that any teacher can create any activity, and then we have to decide, well, is that….

94

00:12:49\.040 \-\-\> 00:12:56\.230

Scott Cytacki: a official one that anybody can find, or not. So we'd have those kind of issues.

95

00:12:56\.330 \-\-\> 00:12:59\.509

Scott Cytacki: Whereas I suppose these would… none of these would be official, but…

96

00:12:59\.940 \-\-\> 00:13:05\.059

Scott Cytacki: You could take a template and have it generate one for you, and that's sort of official.

97

00:13:05\.950 \-\-\> 00:13:06\.580

Chad Dorsey: Nuh.

98

00:13:07\.130 \-\-\> 00:13:15\.920

Leslie Bondaryk (she/her): Yeah, there's an interesting question to answer about whether what you want is always dynamic.

99

00:13:16\.220 \-\-\> 00:13:29\.320

Leslie Bondaryk (she/her): Or is only dynamic until, like, Heisenberg, right? It's the… it's the Heisen content. Is it dynamic until the moment that you view it, and then you know its state, and then it stays there forever?

100

00:13:29\.320 \-\-\> 00:13:30\.170

Chad Dorsey: Right.

101

00:13:30\.170 \-\-\> 00:13:47\.400

Leslie Bondaryk (she/her): Or do you regenerate it every time you open up that problem, which means that you can take into account anything new that you've learned in the meantime. The version of this that we are building in Clue right now is the second of those two, right? It will generate, like.

102

00:13:47\.480 \-\-\> 00:13:50\.240

Leslie Bondaryk (she/her): Here are some other things you could try.

103

00:13:51\.350 \-\-\> 00:13:55\.619

Leslie Bondaryk (she/her): Based on the stuff that's in this suggestion that we're about.

104

00:13:55\.620 \-\-\> 00:13:57\.100

Chad Dorsey: to give you.

105

00:13:57\.100 \-\-\> 00:14:02\.390

Leslie Bondaryk (she/her): And you will get a new version of that every time you open that suggestion.

106

00:14:02\.390 \-\-\> 00:14:03\.070

Chad Dorsey: ….

107

00:14:03\.440 \-\-\> 00:14:05\.190

Leslie Bondaryk (she/her): And that's okay, given…

108

00:14:05\.370 \-\-\> 00:14:21\.720

Leslie Bondaryk (she/her): what we're doing there, but, you know, I could see other flavors of this, like the, you know, I want a… I want a, you know, a tectonic explorer problem that is suitable for teaching, you know, in middle school in Georgia.

109

00:14:21\.720 \-\-\> 00:14:22\.650

Chad Dorsey: And….

110

00:14:22\.650 \-\-\> 00:14:25\.210

Leslie Bondaryk (she/her): Understands the needs of rural communities.

111

00:14:25\.880 \-\-\> 00:14:40\.059

Leslie Bondaryk (she/her): I don't want that problem to change every time, right? The teacher will back and forth, maybe, with it, and then once it's that way, like, it needs to stay that way, and everybody in the class needs to get the same version of that problem, or it's not.

112

00:14:40\.060 \-\-\> 00:14:41\.860

Chad Dorsey: unfair.

113

00:14:42\.850 \-\-\> 00:14:48\.940

Leslie Bondaryk (she/her): So, like, there's… there's some stuff around that, but… Here's the thing.

114

00:14:50\.750 \-\-\> 00:14:54\.039

Leslie Bondaryk (she/her): All of these things are really about the same thing.

115

00:14:54\.040 \-\-\> 00:14:57\.489

Chad Dorsey: Issue that's going on in the industry right now.

116

00:14:57\.490 \-\-\> 00:15:07\.720

Leslie Bondaryk (she/her): Which is that everybody goes, hooray! Large language models are trained for us, right? We didn't have to develop the model, and we didn't have to find the training data, because they did all that for us.

117

00:15:07\.720 \-\-\> 00:15:08\.660

Chad Dorsey: And they keep going.

118

00:15:08\.660 \-\-\> 00:15:22\.780

Leslie Bondaryk (she/her): it, right? They keep trying to get better at it and better at it, so the language gets more natural and all of that. And it completely ignores the fact that unless all you're doing is just chatting about stuff that you could have searched for

119

00:15:22\.780 \-\-\> 00:15:35\.289

Leslie Bondaryk (she/her): basically online anyway, because that's where the training data came from, then you have to retrain these systems, right? And that is every custom application that is worth anything

120

00:15:35\.310 \-\-\> 00:15:45\.050

Leslie Bondaryk (she/her): Is because they are retraining it to understand some particular vector for which they have access to custom training data, right?

121

00:15:46\.360 \-\-\> 00:15:55\.650

Chad Dorsey: Training takes time, training takes money, training takes storage, right? And then you have to solve all these issues of access and speed.

122

00:15:55\.650 \-\-\> 00:16:10\.770

Leslie Bondaryk (she/her): And, you know, and prompting in a system where you don't, you know, a student isn't going to write a good prompt that's going to get them, right? So we have to, like, imagine what the right catch\-all prompt will be.

123

00:16:10\.770 \-\-\> 00:16:20\.500

Chad Dorsey: That will get them what they want. And so, really, we are continuously rebuilding the AI system in some image or another.

124

00:16:20\.720 \-\-\> 00:16:25\.949

Leslie Bondaryk (she/her): Only the perception is that all of that work should already have been done.

125

00:16:25\.950 \-\-\> 00:16:26\.360

Chad Dorsey: Right?

126

00:16:26\.360 \-\-\> 00:16:39\.039

Leslie Bondaryk (she/her): I think this is the problem that's gonna exist when you try to propose new systems. You know, and everybody's going to go, well, why do you need to spend any money on that at all? Why is this a hard thing? You're already done. LLMs exist.

127

00:16:42\.200 \-\-\> 00:16:50\.640

Chad Dorsey: Yeah, I think that's… yes, I think that's true, and I also think that there are ways to… to leverage…

128

00:16:50\.860 \-\-\> 00:17:07\.570

Chad Dorsey: the… I mean, recognize what you have, and double down on that part in some cases. So, I mean, for example, when you're teaching science, there's a lot that's known about the world of science, and the ways that things in it operate, and the entities, and the somethings, and the whatevers.

129

00:17:07\.569 \-\-\> 00:17:18\.199

Chad Dorsey: And, when you're dealing with simulations, there are quantities that are in the simulation and quantities that aren't in the simulation, and you can only work with the ones that are in there, so there are, there are sort of…

130

00:17:18\.359 \-\-\> 00:17:22\.370

Chad Dorsey: Objects, entities, relationships, things that you can, …

131

00:17:22\.510 \-\-\> 00:17:38\.180

Chad Dorsey: avail yourself of, and you can… and you can lean back and say, you know, see natural world, you know, in parentheses, and then get something back, and then work with that. So, you know, and there may be sub… other versions of that, sort of, like.

132

00:17:38\.180 \-\-\> 00:17:45\.569

Chad Dorsey: Where do we tap on the shoulder of the thing that knows something that we know is important, and then pull it from my, you know, the…

133

00:17:45\.570 \-\-\> 00:17:51\.240

Chad Dorsey: working model I've been, you know, playing with in my runs or whatever is, you know, just, like.

134

00:17:51\.720 \-\-\> 00:18:11\.480

Chad Dorsey: eco… like, the customizing an ecosystem for something, like, you name 4 or 5, you know, animals and plants and some things around you, and I could probably build you an ecosystem that operates like an ecosystem operates, and, like, really, it operates like that, probably, you know, and there are end versions of that sort of thing, or…

135

00:18:11\.690 \-\-\> 00:18:14\.580

Chad Dorsey: you know… I may be able to say.

136

00:18:14\.580 \-\-\> 00:18:36\.300

Chad Dorsey: I'm in whatever state, and I want my students today to think about igneous rocks, set up something that's gonna have them stumble on igneous rocks with a mountain that sort of looks like mine, right? You know, and 5 different versions of that for 5 different groups is fine, too, you know, something. So, there are, I think, some ways to say, you know.

137

00:18:36\.300 \-\-\> 00:18:47\.800

Chad Dorsey: only those dials in the ecosystem, you can deal with those dials, because we know you're not going to get the wrong physics equations, or something like that. So that's one subset it feels like we might be able to lean on.

138

00:18:50\.280 \-\-\> 00:19:00\.859

Scott Cytacki: The other thing I'd add is, like, something that Leslie posted about, which is the training part. There's been ways of using LMs to improve that as well, or vector embedding stuff to…

139

00:19:01\.040 \-\-\> 00:19:10\.489

Scott Cytacki: Sort of group the set of data that you need to look at, so… into smaller clusters, and then you… you would identify

140

00:19:11\.180 \-\-\> 00:19:24\.930

Scott Cytacki: exemplars from each cluster and just train on that, which then reduces your set, I forget what the number was, like, more than 10%, maybe 1% or something. So it makes this whole idea of, like, tagging and training much more reasonable, and…

141

00:19:25\.180 \-\-\> 00:19:26\.790

Scott Cytacki: And I think it has a good…

142

00:19:26\.810 \-\-\> 00:19:45\.739

Scott Cytacki: approach for even, like, within classroom, like, small numbers of documents where we already talked about this idea of, like, identifying similar things and then picking an exemplar from one of those similar things so that the teacher could throw up on the screen, like, four things that represent the four different ways the class is thinking about a problem.

143

00:19:45\.920 \-\-\> 00:19:52\.399

Scott Cytacki: So it's the same… same kind of issue that… I don't think we really explored that much, this whole sort of identifying clusters, then…

144

00:19:53\.670 \-\-\> 00:19:57\.810

Scott Cytacki: getting an exemplar from the cluster. So… Yep.

145

00:20:00\.780 \-\-\> 00:20:02\.980

Chad Dorsey: What's the…

146

00:20:02\.980 \-\-\> 00:20:20\.990

Chad Dorsey: So, what's the larger ground that we're… you know, we're talking about different building designs or whatever, but, you know, we haven't recognized that we're in a mountainous region and need to find a flat place to build something? You know, what's the… what's the thing that we… things that we need to think about across, that would

147

00:20:20\.990 \-\-\> 00:20:27\.410

Chad Dorsey: that would, you know, supercharge or open up opportunities, for this something. Like, oh yeah, that's fine, but…

148

00:20:27\.450 \-\-\> 00:20:42\.299

Chad Dorsey: you know, Jay's just gonna build it in the corner of some system or whatever, and then we're not gonna be able to really leverage that idea across everything, or if we've built a mega\-massive, you know, activity extraordinaire, I don't know, right?

149

00:20:50\.850 \-\-\> 00:20:53\.690

Scott Cytacki: Yeah, I don't know. I mean, I don't know exactly what she's planning to build tonight.

150

00:20:53\.690 \-\-\> 00:20:54\.740

Chad Dorsey: No, nobody, I don't know.

151

00:20:54\.740 \-\-\> 00:20:55\.180

Scott Cytacki: They're interested.

152

00:20:55\.180 \-\-\> 00:21:06\.270

Chad Dorsey: I'm just, no, I'm just saying, you know, I can imagine lots of cases where we cater to one particular PI in a corner or something, and then don't, you know, create a situation where we can generalize. But if we know

153

00:21:06\.300 \-\-\> 00:21:26\.269

Chad Dorsey: really, it's all about REG and this contextualized training, then, you know, do we need a layer for that? Is it, you know, what about, you know, we double down on memory and recognize that we actually need to make sure that we're always building in hooks from memory everywhere, and it's not going to pay off for a year or two, but then all of a sudden, we've got it, you know?

154

00:21:27\.270 \-\-\> 00:21:27\.950

Leslie Bondaryk (she/her): I…

155

00:21:31\.060 \-\-\> 00:21:38\.079

Leslie Bondaryk (she/her): I don't know that this is all of it, but this is definitely a piece of it. The thing that we have been…

156

00:21:38\.350 \-\-\> 00:21:53\.690

Leslie Bondaryk (she/her): struggling with, honestly, for several years now with Clue. And, you know, and I just, I feel like maybe we've finally started to crack this problem, is that there are different representations for things.

157

00:21:53\.690 \-\-\> 00:21:58\.460

Leslie Bondaryk (she/her): Right? And sometimes you care about the description, sometimes you care about a picture.

158

00:21:58\.460 \-\-\> 00:21:58\.800

Chad Dorsey: Hmm.

159

00:21:58\.800 \-\-\> 00:22:00\.620

Leslie Bondaryk (she/her): Right, a graph or something.

160

00:22:00\.670 \-\-\> 00:22:07\.180

Leslie Bondaryk (she/her): Right? In the same way that you have all these ways of doing things when you're solving problems. Right.

161

00:22:07\.180 \-\-\> 00:22:25\.259

Leslie Bondaryk (she/her): Right? Sometimes it's an expression in math, sometimes it's a description in the lab book, sometimes it's a graph, or a little sketch of this scenario, or a photo, or whatever, right? You need to have… you need to not bar yourself from having ways to represent that

162

00:22:25\.430 \-\-\> 00:22:32\.770

Leslie Bondaryk (she/her): information in multiple formats so that you can feed it to the LLM, so you can feed it to the system.

163

00:22:32\.770 \-\-\> 00:22:49\.559

Leslie Bondaryk (she/her): Right? All of… it sounds like all the rest of this, all the rest of this, like, wiring is the hard part. It's not. It's having something that the LLM can read, and often something that the LLM can read fast, or at least consistently.

164

00:22:49\.600 \-\-\> 00:22:50\.540

Leslie Bondaryk (she/her): ….

165

00:22:50\.540 \-\-\> 00:22:58\.649

Chad Dorsey: Yeah, which is why radio… radiography and whatever, I mean, or spectrograms, and people were like, oh, they're pictures, you know, oh, okay, yeah, right.

166

00:22:58\.650 \-\-\> 00:23:07\.590

Leslie Bondaryk (she/her): Right. We have over the years, and I've been fighting this, but I never seem to quite win this battle, because it's always expensive.

167

00:23:08\.680 \-\-\> 00:23:29\.519

Leslie Bondaryk (she/her): we have standardized on this notion of the snapshot, particularly Amy's group, because they really like the activity player structure of doing things, and it, you know, that was a facility that has existed there for a long time. So we, like, resuscitate that snapshot server, and that is what we lean on. Oh, I'll just let them make a picture, and they can scribble on top of it.

168

00:23:30\.210 \-\-\> 00:23:39\.819

Leslie Bondaryk (she/her): you know, we'll need that. We'll need those snapshots, or at least the ability to generate them, maybe behind the scenes, maybe late at night, you know, in a big batch.

169

00:23:40\.520 \-\-\> 00:23:45\.800

Leslie Bondaryk (she/her): But you also need the state, right? You need that state management.

170

00:23:45\.800 \-\-\> 00:23:46\.940

Chad Dorsey: Thousand percent.

171

00:23:46\.940 \-\-\> 00:24:00\.910

Leslie Bondaryk (she/her): And you need, you know, some kind of strategy for… for spitting it out as a description, and some kind of strategy for spitting it out as a blah blah blah, right? And guess what? You also care about those things for accessibility.

172

00:24:02\.850 \-\-\> 00:24:03\.530

Leslie Bondaryk (she/her): Right?

173

00:24:03\.700 \-\-\> 00:24:04\.150

Chad Dorsey: Nope.

174

00:24:04\.150 \-\-\> 00:24:04\.790

Leslie Bondaryk (she/her): and….

175

00:24:04\.790 \-\-\> 00:24:13\.410

Scott Cytacki: And there's interesting problems there of, like, you know, just sending it the JSON description of the activity, or the student's answer of something like a drawing.

176

00:24:13\.540 \-\-\> 00:24:21\.579

Scott Cytacki: doesn't work well, whereas for drawing, a picture works better than the JSON description, but then something like a graph

177

00:24:21\.710 \-\-\> 00:24:27\.720

Scott Cytacki: Probably having both the picture and the table of the data that the graph is graphing would be the best representation.

178

00:24:27\.910 \-\-\> 00:24:45\.930

Scott Cytacki: But we don't have a ton of experience with that yet, I don't think. But I would… I would… that's one way I'd summarize this, is, like, anything new we create, we should make it in a way that AI can understand it. That it could… it could describe back this thing that you just created to you in its works.

179

00:24:45\.960 \-\-\> 00:24:46\.800

Chad Dorsey: Yum.

180

00:24:46\.800 \-\-\> 00:24:47\.480

Scott Cytacki: Hmm, doesn't.

181

00:24:47\.480 \-\-\> 00:25:01\.729

Chad Dorsey: Yeah, I was… I was… yeah, Dan and I were talking about AI browsers last night, and, you know, trying stuff out, and I was describing that, you know, Comet did okay for me, but then I, you know, went to the OmniFocus site, and it was like, make me a task, and…

182

00:25:01\.730 \-\-\> 00:25:15\.850

Chad Dorsey: you know, it sort of saw the plus sign up there, and tried to find it, but it couldn't find the tag for it, and there's a keyboard command, so it tried to press N, but it couldn't press N, and it tried to generate JavaScript sign, and I was like, oh, like.

183

00:25:15\.910 \-\-\> 00:25:24\.869

Chad Dorsey: AI\-first design was, you know, is not in the picture for this in any way, shape, or form. And yes, accessibility, you know, plus plus, right?

184

00:25:25\.010 \-\-\> 00:25:36\.880

Leslie Bondaryk (she/her): Right, I mean, one of the things that we're doing on clue documents, because we need this solution right now, is having an export format that is defined for every tile type.

185

00:25:36\.880 \-\-\> 00:25:37\.230

Chad Dorsey: And they're….

186

00:25:37\.230 \-\-\> 00:25:56\.339

Leslie Bondaryk (she/her): different, right? Textiles, obviously, we need to shunt all the JSON cruft out of the way, and all the formatting cruft out of the way, right? And just hand you a blob of text. We use Markdown, but okay, whatever. Tables actually turn out to work pretty well that way, too.

187

00:25:57\.110 \-\-\> 00:26:03\.660

Leslie Bondaryk (she/her): expressions, you can export, as it turns out, you can export LaTeX, and….

188

00:26:03\.660 \-\-\> 00:26:06\.840

Chad Dorsey: that's fine, right? Any kind of, like, markup.

189

00:26:07\.070 \-\-\> 00:26:14\.310

Leslie Bondaryk (she/her): that shows you that it is math, and what the hierarchy and the expression is, is actually quite useful. So those

190

00:26:14\.820 \-\-\> 00:26:17\.390

Leslie Bondaryk (she/her): Okay. Graphs?

191

00:26:17\.390 \-\-\> 00:26:18\.960

Chad Dorsey: Interesting, really.

192

00:26:19\.320 \-\-\> 00:26:29\.860

Leslie Bondaryk (she/her): Yeah, so we gotta do something different for graphs, and I have a story for it, we haven't gotten there yet. I'm kind of hoping that, that the project with Edo will pay for that, right? But, you know, so there's….

193

00:26:29\.860 \-\-\> 00:26:35\.920

Chad Dorsey: Because you're talking about graphs as graphics, and the only other option is a really big data set, or, I mean….

194

00:26:35\.920 \-\-\> 00:26:38\.960

Leslie Bondaryk (she/her): Mostly just because we haven't worked on it yet. We'll find something.

195

00:26:38\.960 \-\-\> 00:26:41\.260

Chad Dorsey: Oh, I see, you're saying graphs include so on, okay.

196

00:26:41\.260 \-\-\> 00:26:49\.929

Leslie Bondaryk (she/her): Yeah, graphs include, and we tried to, like, put… spit out the most obvious, minimal description of what's in there, and it didn't really…

197

00:26:50\.200 \-\-\> 00:26:57\.359

Leslie Bondaryk (she/her): It doesn't really tell you anything. Oh, and guess what? This is the same problem that you have when you're blind. Exactly.

198

00:26:57\.360 \-\-\> 00:27:06\.929

Chad Dorsey: It's the same problem that everybody's been trying to work on for years. How do you describe a thing that is meant to be a visual representation to a system or a.

199

00:27:06\.930 \-\-\> 00:27:10\.749

Leslie Bondaryk (she/her): person who does not work in visual representation, right?

200

00:27:10\.750 \-\-\> 00:27:11\.779

Chad Dorsey: And, yep.

201

00:27:12\.040 \-\-\> 00:27:25\.550

Leslie Bondaryk (she/her): But it can't, right? But they can, we just haven't, you know, like, we haven't worked on it yet. It's a problem to work on. But if you can't make the stuff that you are doing, or that the student is doing.

202

00:27:25\.550 \-\-\> 00:27:30\.789

Chad Dorsey: quick to evaluate, and you can't do anything very interesting with it, and that.

203

00:27:30\.860 \-\-\> 00:27:38\.410

Leslie Bondaryk (she/her): that problem is subtle. It's not the same for every type of information, and we haven't even begun.

204

00:27:38\.410 \-\-\> 00:27:42\.650

Chad Dorsey: to dig into the problem of linked representations, right?

205

00:27:42\.650 \-\-\> 00:27:46\.059

Leslie Bondaryk (she/her): How do I know about what the student knows if they have

206

00:27:46\.410 \-\-\> 00:27:51\.989

Leslie Bondaryk (she/her): Graphed a dataset that they created as a table, and then created a graph.

207

00:27:52\.830 \-\-\> 00:28:06\.760

Leslie Bondaryk (she/her): What do I know about what they know if they have put sparrows on that thing, right? What do I know if they started with the graph representation of the points and translated that into a table? Blah blah, blah, blah, blah, blah.

208

00:28:07\.420 \-\-\> 00:28:12\.930

Leslie Bondaryk (she/her): So it's a… it's a big problem, but it's not the only problem.

209

00:28:12\.930 \-\-\> 00:28:14\.810

Chad Dorsey: Right?

210

00:28:15\.000 \-\-\> 00:28:21\.869

Leslie Bondaryk (she/her): There is also, I think… I mean, when we were talking about the Google thing, we talked a lot about

211

00:28:21\.930 \-\-\> 00:28:27\.950

Leslie Bondaryk (she/her): You know, how are we really gonna do this in two years for a large number of things?

212

00:28:27\.960 \-\-\> 00:28:46\.920

Leslie Bondaryk (she/her): And my answer at the time for that is, well, we're going to do a thing that's kind of like what we did for DESE. We're going to restrict the format, we're going to make it really obvious what the variables are that you can change, and then we'll scaffold based on number and complexity of the interactions of those variables.

213

00:28:48\.300 \-\-\> 00:28:50\.519

Leslie Bondaryk (she/her): I mean, maybe that's good?

214

00:28:51\.900 \-\-\> 00:28:59\.919

Leslie Bondaryk (she/her): maybe it's not enough. I don't really know, we haven't worked on this problem yet. But… but I sense that there is some kind of…

215

00:29:00\.560 \-\-\> 00:29:09\.900

Leslie Bondaryk (she/her): You know, problem schema, or simulation schema that we're gonna have to standardize on in order to extend the notion.

216

00:29:12\.060 \-\-\> 00:29:15\.540

Leslie Bondaryk (she/her): And it's a thing that we don't, like, we haven't really worked on.

217

00:29:16\.430 \-\-\> 00:29:21\.549

Leslie Bondaryk (she/her): But we… but we did for Desi, only we didn't do most of the psychometric work there.

218

00:29:22\.130 \-\-\> 00:29:28\.559

Leslie Bondaryk (she/her): Right? And that's actually what you're talking about doing here, right? You're talking about taking on that problem of

219

00:29:29\.140 \-\-\> 00:29:35\.430

Leslie Bondaryk (she/her): of… Again, coming up with a representation of the interaction patterns that can be achieved.

220

00:29:35\.430 \-\-\> 00:29:46\.440

Chad Dorsey: Yeah, that's what I was going to ask. So how does this get into, essentially, process and student experience to capture? Because artifacts are one part of that, but obviously not the whole. Right.

221

00:29:46\.440 \-\-\> 00:29:50\.660

Leslie Bondaryk (she/her): Right, you know, it's honestly not…

222

00:29:50\.880 \-\-\> 00:30:15\.860

Leslie Bondaryk (she/her): We feel like we're making… we're making strides against the multiple choice problem, but we're really not, right? We're really just… we're really just masking it to a greater extent, but I'm still talking about trying to boil the thing down to a consistent enough pattern of stuff that we can recognize it over and over again, regardless of what's inside the choices on my multiple choice question. Okay, so now I have a bunch of buttons

223

00:30:15\.860 \-\-\> 00:30:21\.089

Leslie Bondaryk (she/her): That look like they allow you to do elaborate things with, you know, with values inside of the simulation.

224

00:30:21\.220 \-\-\> 00:30:25\.430

Leslie Bondaryk (she/her): But honestly, I'm asking you to boil it down to a multiple\-choice problem again.

225

00:30:26\.590 \-\-\> 00:30:27\.270

Chad Dorsey: Hmm.

226

00:30:29\.200 \-\-\> 00:30:29\.720

Chad Dorsey: Yeah.

227

00:30:29\.720 \-\-\> 00:30:42\.789

Scott Cytacki: Yeah, I mean, I was… I mean, related to that somewhat is when we're talking about how to represent things so that the LLM can understand them. The thing we didn't talk about was, like, motion stuff. Like, whether it's a simulation or you're making an animation yourself.

228

00:30:42\.940 \-\-\> 00:30:51\.329

Scott Cytacki: that's even harder than the graph stuff, and I think you kind of just have to wait on that, other than providing textual descriptions, but…

229

00:30:51\.960 \-\-\> 00:30:55\.009

Scott Cytacki: I don't know, I haven't tried yet the…

230

00:30:55\.320 \-\-\> 00:31:02\.539

Scott Cytacki: well, one, I think it'd be very expensive for us to use, but I haven't tried, like, describe this video to me in AI terms, but that would.

231

00:31:02\.540 \-\-\> 00:31:06\.660

Chad Dorsey: I think. I haven't eaten too much, but, yeah, but I don't know if…

232

00:31:08\.050 \-\-\> 00:31:17\.819

Chad Dorsey: I don't know how much that's tracked object from one place to another. I mean, it might be, yeah, interesting. It's worth tracking that as a way to layer that in.

233

00:31:18\.540 \-\-\> 00:31:37\.689

Scott Cytacki: Because, like, you can boil down some of our things, like the DESI or the FOSS ones more than others, where they are just, like, a few toggles that you can change, and then you could give that information to the AI and describe what's happening, and then it could know what's going on. But for more complex ones, where you can just drag an atom here or there, or you… Right.

234

00:31:38\.200 \-\-\> 00:31:47\.619

Scott Cytacki: you've got an infinite amount of possibilities of things you could do, and make it do when you're done editing it, so then you'd want the AI to know what the output was somehow.

235

00:31:47\.930 \-\-\> 00:32:02\.020

Chad Dorsey: Yeah, how do I know what your… what your experience path in the Tectonic Explorer is well enough to understand what I could ask you, or supplement, or what have you, right? I mean, usually we…

236

00:32:02\.480 \-\-\> 00:32:16\.759

Chad Dorsey: we've sort of backfilled that by forcing you to create some artifact, which is still probably part of the picture, create some artifact that depends upon your experience enough that we can infer it from… from that. But…

237

00:32:16\.970 \-\-\> 00:32:17\.979

Chad Dorsey: I mean, it…

238

00:32:18\.260 \-\-\> 00:32:37\.749

Chad Dorsey: it could be… I mean, the answer, at least the short\-term answer, is probably some variation of that, like, well, you couldn't have seen the back of the world at this point, and you did have these things on your screen, and we're only attending to 4 of them, because those are the ones that really matter, so…

239

00:32:37\.750 \-\-\> 00:32:42\.300

Chad Dorsey: Did you click anywhere near Object 1, or, you know, something like that, right?

240

00:32:42\.300 \-\-\> 00:32:55\.800

Leslie Bondaryk (she/her): Yeah, so it's this two\-part problem. One is, you know, boiling down a description, whatever that means, whether it's text, or images, or some combination, or a markup language, or something else, I don't know.

241

00:32:55\.800 \-\-\> 00:33:09\.490

Leslie Bondaryk (she/her): And two is summarizing it, right? You know, LLMs have a language\-focused way of providing summaries, but for example, think about the work that we did with Dan, I don't know.

242

00:33:09\.640 \-\-\> 00:33:25\.499

Leslie Bondaryk (she/her): at this point, a long time ago, it was a long time ago, where we were trying to come up with some representation of SAGE models that we could display to teachers in a dashboard that would be better than just plopping, you know, 27 models in a big column.

243

00:33:25\.500 \-\-\> 00:33:26\.010

Chad Dorsey: Right.

244

00:33:26\.010 \-\-\> 00:33:33\.690

Leslie Bondaryk (she/her): Right? What can we say about those diagrams that is helpful in understanding what the students are doing?

245

00:33:33\.690 \-\-\> 00:33:34\.280

Chad Dorsey: And we….

246

00:33:34\.580 \-\-\> 00:33:41\.680

Leslie Bondaryk (she/her): a little visual language, and a bunch of metrics, right? They were like, I forget what, 6 different classifications.

247

00:33:42\.040 \-\-\> 00:33:51\.689

Leslie Bondaryk (she/her): right, doing that kind of work is really what we're talking about here. How do we classify sims? How do we classify experimentation and inquiry.

248

00:33:51\.690 \-\-\> 00:33:52\.190

Chad Dorsey: Yes.

249

00:33:52\.200 \-\-\> 00:33:53\.630

Leslie Bondaryk (she/her): ways that…

250

00:33:53\.920 \-\-\> 00:34:05\.130

Leslie Bondaryk (she/her): The teacher can understand the summary, that the student can understand that as feedback, and say, well, you've only done, you know, 5 of these 6 things, maybe you want to try number 6\.

251

00:34:06\.110 \-\-\> 00:34:06\.909

Chad Dorsey: And, you know.

252

00:34:06\.980 \-\-\> 00:34:17\.239

Leslie Bondaryk (she/her): and so that the LLM can then offer those suggestions, because it's got enough experience with similar coding, right? It's all back to the rubric, you know. Yep.

253

00:34:17\.340 \-\-\> 00:34:30\.629

Leslie Bondaryk (she/her): you know, I'm dissing it a little bit, but honestly, it still matters, right? Right? And what is that rubric? Right now, my rubrics are language\-based, because guess what? I have language\-based models at my

254

00:34:31\.550 \-\-\> 00:34:35\.560

Leslie Bondaryk (she/her): But… They need to… they need to reach beyond that.

255

00:34:35\.560 \-\-\> 00:34:36\.350

Chad Dorsey: Right.

256

00:34:36\.350 \-\-\> 00:34:45\.010

Leslie Bondaryk (she/her): We need to develop these classification systems that, you know, is it number of variables? That's easy for me to understand, so I'm.

257

00:34:45\.010 \-\-\> 00:34:45\.350

Chad Dorsey: testing.

258

00:34:45\.350 \-\-\> 00:34:52\.920

Leslie Bondaryk (she/her): That might be one of the ways that you make things harder, more interesting. But maybe that's wrong, right? I don't know.

259

00:34:52\.929 \-\-\> 00:34:56\.279

Chad Dorsey: Yeah, I mean, the thing… the thing that you really…

260

00:34:56\.429 \-\-\> 00:35:07\.029

Chad Dorsey: want to do, or an approach, is… is, I guess, sort of training\-related. I mean, essentially, you're talking about

261

00:35:07\.829 \-\-\> 00:35:31\.309

Chad Dorsey: about, you know, a parameter space of configurations, which is, you know, huge but not infinite, you know, for a simulation or what have you, and the parameter space of, you know, usual configurations is, you know, small or unimaginable. You know, when the thing is up high, then you're probably only, you know, tending to these parts of the… when the slider is down low, so there's some set of parameter

262

00:35:31\.309 \-\-\> 00:35:32\.409

Chad Dorsey: space…

263

00:35:32\.419 \-\-\> 00:35:47\.359

Chad Dorsey: And simulations state, you know, states that exist and are presented, and there's some set of interactions that you're doing in time along with those, and so you end up with this n\-dimensional space of

264

00:35:47\.709 \-\-\> 00:36:10\.569

Chad Dorsey: combinations of interaction patterns and simulation states, and what you're interested in are the hot spots within that space. Like, oh, okay, like, this one is associated with, you know, igneous rock production and questions X, Y, and Z around something. This one's associated with, you know, cooling and albedo effects, and this one's sort of like… and…

265

00:36:10\.569 \-\-\> 00:36:14\.769

Chad Dorsey: And one could imagine, you know, I don't know how to create, but one could imagine

266

00:36:14\.859 \-\-\> 00:36:34\.439

Chad Dorsey: populating a dataset that had enough of, like, students swimming in this parameter space to be able to say, okay, like, we're gonna tune for this, or this, or this, and I see where you are, you know, the simulation's here, but you're over here, you know, we want you to be sort of closer to that good question state.

267

00:36:34\.439 \-\-\> 00:36:35\.779

Chad Dorsey: Now that's all…

268

00:36:35\.779 \-\-\> 00:36:41\.239

Chad Dorsey: swimming, but right now, we don't have the data to even create that, you know, crazy Blade Runner thing.

269

00:36:41\.760 \-\-\> 00:36:43\.389

Leslie Bondaryk (she/her): Right. And…

270

00:36:44\.400 \-\-\> 00:37:01\.860

Leslie Bondaryk (she/her): And that rubric that we've developed is very much driven at this point by me trying my best to encapsulate what an experienced CMP teacher does when they summarize documents, right? What are they doing? I was… this was the brief.

271

00:37:01\.860 \-\-\> 00:37:10\.709

Leslie Bondaryk (she/her): Right? How do we improve a teacher's ability to conduct those summary sessions in a productive way in a digital environment?

272

00:37:11\.180 \-\-\> 00:37:15\.249

Leslie Bondaryk (she/her): This was the solution that I came up with for that. And…

273

00:37:15\.920 \-\-\> 00:37:19\.810

Leslie Bondaryk (she/her): And so I need to understand, and I don't yet.

274

00:37:20\.190 \-\-\> 00:37:43\.969

Leslie Bondaryk (she/her): what science… inquiry\-based science classroom teachers do when they are trying to teach kids using simulations, right? What do they… what actions do they want them to do? How do they scaffold them? What… what milestones are they looking for on that student journey? How do they know when a kid is ready for the next set of activities?

275

00:37:43\.970 \-\-\> 00:37:52\.529

Leslie Bondaryk (she/her): Maybe the UI gives them everything it wants, and it doesn't matter at all, because what the teacher is telling them is, you know, well, try this now, right?

276

00:37:52\.990 \-\-\> 00:38:06\.219

Leslie Bondaryk (she/her): I don't know how that journey goes yet, and I don't know how to get that information. But once I have it, then, you know, then our challenge will be similarly, just like we've done with the tagging and so on, so far in Clue.

277

00:38:06\.660 \-\-\> 00:38:13\.899

Leslie Bondaryk (she/her): to figure out a way to codify it and get… get the… get the LLM to recognize it, right?

278

00:38:14\.610 \-\-\> 00:38:20\.620

Leslie Bondaryk (she/her): Now, how do we store it? How do we express it? How do we recognize patterns in it? And how do we

279

00:38:20\.620 \-\-\> 00:38:36\.360

Leslie Bondaryk (she/her): And how do we develop a UI that talks about those patterns in a way that a teacher, at least, and hopefully a teacher and a student, will immediately recognize what's going on, you know, and go, oh yeah, now I want, you know, button 3, right?

280

00:38:45\.640 \-\-\> 00:38:48\.270

Scott Cytacki: I don't know if we're ready to talk about other stuff, too, or….

281

00:38:48\.270 \-\-\> 00:38:49\.170

Chad Dorsey: Right.

282

00:38:49\.170 \-\-\> 00:38:56\.370

Scott Cytacki: I mean, just one, like, having… learning… learning about this stuff for the MW work makes me think of what

283

00:38:56\.790 \-\-\> 00:39:00\.410

Scott Cytacki: We could be doing for students to learn new things, too.

284

00:39:00\.410 \-\-\> 00:39:09\.450

Chad Dorsey: And my, you know, my interaction with it is, like, asking questions, and then it suggests, oh, do you want me to make a graph of that? And, like, oh, sure, give me a graph, and then that helps, but….

285

00:39:09\.450 \-\-\> 00:39:10\.370

Scott Cytacki: It's this…

286

00:39:11\.060 \-\-\> 00:39:27\.069

Scott Cytacki: interesting sort of driving, where I'm asking questions in order to get answers that help me learn what I want to know, which is different than what we do. We make activities that guide them in directions, as opposed to them asking questions.

287

00:39:27\.580 \-\-\> 00:39:43\.199

Scott Cytacki: And so I don't know, it feels like there's something there that's possible now that we, you know, we want teachers to do, but we could do with AI more now, and we were nervous about it because we didn't want AI to, like, say inappropriate things to students, but…

288

00:39:43\.200 \-\-\> 00:39:53\.770

Scott Cytacki: I think it's doable more now, safer now, that it seems like we should try something along those lines, where it's flipping around so that the students driving

289

00:39:53\.790 \-\-\> 00:39:57\.830

Scott Cytacki: But then we now need to figure out how to evaluate whether or not they are

290

00:39:58\.780 \-\-\> 00:40:04\.320

Scott Cytacki: driving in the right direction, I guess, or like… I don't know, I'm not sure what we can do with it to….

291

00:40:04\.320 \-\-\> 00:40:05\.100

Chad Dorsey: It's screamed.

292

00:40:05\.100 \-\-\> 00:40:06\.189

Scott Cytacki: appropriate versus just.

293

00:40:06\.190 \-\-\> 00:40:13\.990

Chad Dorsey: You're saying enabling more open investigation that is mediated by LLMs and the tools that we have.

294

00:40:14\.510 \-\-\> 00:40:31\.649

Scott Cytacki: Yeah, and I mean, maybe it can be directed in the sense that, yes, this is the topic that you're supposed to understand, and you can understand it by just asking the AI to describe it to you, but does that really mean you understand it? So then it's a matter of us trying to build things to make sure that you actually understand it, or

295

00:40:31\.800 \-\-\> 00:40:32\.730

Scott Cytacki: I guess…

296

00:40:33\.310 \-\-\> 00:40:39\.960

Scott Cytacki: Maybe. And a way to do that would be to challenge them somehow to… I guess it means an assessment again, where….

297

00:40:39\.960 \-\-\> 00:40:40\.360

Chad Dorsey: Hmm.

298

00:40:41\.010 \-\-\> 00:40:45\.380

Scott Cytacki: But as opposed to doing all that by a canned thing, it's more…

299

00:40:45\.830 \-\-\> 00:40:49\.340

Scott Cytacki: Interactively driven by the student asking questions.

300

00:40:49\.750 \-\-\> 00:40:57\.870

Chad Dorsey: So you wonder if there's, you know, a first pass at this that, you know, might set things up. I mean, basically, …

301

00:40:58\.230 \-\-\> 00:41:17\.470

Chad Dorsey: you could imagine an LLM that had a… this is what you were describing at the very beginning, as the RAG database of our simulations to draw upon, and say, I see you're asking about, you know, climate change, let me, you know, maybe you want to try this something. And, you know, with

302

00:41:17\.890 \-\-\> 00:41:19\.589

Chad Dorsey: layers beneath about…

303

00:41:19\.590 \-\-\> 00:41:40\.070

Chad Dorsey: you know, some subset of my crazy parameter space of, like, this simulation is good for, you know, X and Y and Z kinds of questions when attuned in these sorts of ways, or maybe even just, like, you can see, you get it, right? We'll expose all the dials for you, and you can try some setup that you think is gonna, you know, end up in the right direction.

304

00:41:40\.070 \-\-\> 00:41:51\.559

Chad Dorsey: So, A, here's a simulation to play with. B, here's a simulation configured to the situation you're asking about, about carbon dioxide being absorbed by the ocean. What do you think is happening here? You know, something like that.

305

00:41:51\.910 \-\-\> 00:41:52\.950

Chad Dorsey: And, ….

306

00:41:52\.950 \-\-\> 00:41:56\.010

Leslie Bondaryk (she/her): Yeah, I mean, you're basically describing Moda.

307

00:41:56\.240 \-\-\> 00:41:57\.819

Chad Dorsey: Right, probably.

308

00:41:57\.820 \-\-\> 00:42:13\.590

Leslie Bondaryk (she/her): Right? You know, where the kid can be given a pre\-configured simulation, right? The code can be written such that it does a thing, and here, we want you to see, run this, we want you to see the way this behaves, and look at the logic that

309

00:42:13\.590 \-\-\> 00:42:37\.420

Leslie Bondaryk (she/her): encodes that behavior. And now, okay, here, here's an open slate. Program it any way you want, make the atoms behave any way you want, right? What is wrong or right about that, you know, is left as an analysis exercise. It's fascinating, actually. I got so much pushback about building that kind of thing while working with the DESI folks, maybe just because it was a

310

00:42:37\.420 \-\-\> 00:42:39\.179

Leslie Bondaryk (she/her): a summative assessment? I don't know.

311

00:42:39\.810 \-\-\> 00:42:47\.159

Leslie Bondaryk (she/her): but probably not, right? You know, they were like, you can't create misconceptions. You mustn't allow the student to generate

312

00:42:47\.480 \-\-\> 00:42:48\.900

Leslie Bondaryk (she/her): Something that is wrong.

313

00:42:49\.480 \-\-\> 00:42:57\.080

Leslie Bondaryk (she/her): I'm like, well, yeah, but if they can't explore what the wrong way is, and notice that it is behaving not in the way that they expected.

314

00:42:57\.080 \-\-\> 00:42:57\.849

Chad Dorsey: And how are they gonna do.

315

00:42:57\.850 \-\-\> 00:42:58\.930

Leslie Bondaryk (she/her): anything, right?

316

00:43:00\.170 \-\-\> 00:43:01\.410

Chad Dorsey: Yeah.

317

00:43:02\.140 \-\-\> 00:43:19\.240

Scott Cytacki: And thinking about it, like, that the assessment part, at least for my explorations, is, like, when I then describe what I've learned, and then Dan comes back, or someone else comes back with a review of it, kind of. So that peer interaction, it could be the assessment where we don't even have to

318

00:43:19\.760 \-\-\> 00:43:31\.139

Scott Cytacki: built some magical assessment system, but it's instead, you learn the concept, or have the AI describe you the concept, and you think you learn it, and now you have to describe that to someone else, and then they evaluate your description.

319

00:43:32\.320 \-\-\> 00:43:51\.669

Chad Dorsey: Yeah, that's actually very interesting. So you could imagine a situation where you give a group of, you know, 3 or 4 students a topic, and they ask questions about it, and then you suggest a simulation, and they explore it, and then the end result is them sort of cross\-trading explanations together with the…

320

00:43:52\.210 \-\-\> 00:44:03\.330

Chad Dorsey: you know, with some version of, you know, here's what I did, you know, like, explainer, video, or, you know, state, state sequence or something.

321

00:44:03\.980 \-\-\> 00:44:09\.030

Leslie Bondaryk (she/her): Oh, okay, but this is, again, this just proves my point even further, right? No, totally.

322

00:44:09\.030 \-\-\> 00:44:33\.779

Leslie Bondaryk (she/her): None of these are techniques, our educational techniques that have not already been discovered, validated, right? Like, what I need is to know which of the best of the teaching techniques we are trying to encode inside of a system. And then I will know, you know, or at least Scott and I and whoever else wants to play can talk ad nauseam about the best ways to.

323

00:44:33\.780 \-\-\> 00:44:40\.060

Chad Dorsey: To make those happen better, or at least digitally.

324

00:44:40\.150 \-\-\> 00:44:49\.730

Leslie Bondaryk (she/her): using these new LLM tools that we've got, right? But… but I haven't done the deep dive into the, you know, into the….

325

00:44:49\.730 \-\-\> 00:44:50\.350

Chad Dorsey: Right.

326

00:44:50\.450 \-\-\> 00:44:54\.169

Leslie Bondaryk (she/her): Right into the backbone of teaching theory.

327

00:44:54\.500 \-\-\> 00:44:57\.949

Leslie Bondaryk (she/her): To know which… to know what we're going after here, right?

328

00:44:57\.950 \-\-\> 00:45:04\.440

Chad Dorsey: Yeah, no, that's totally fair, and, you know, I'm only trying to, maybe unfairly, but no, I don't think so, tease out the

329

00:45:04\.480 \-\-\> 00:45:24\.100

Chad Dorsey: the… the thing that you were doing for Corey, the table… the grand table stakes in all this, of, like, you know, once you… once you come back and say, oh, I see teachers do this, you know, can we be ready to go, oh yeah, that's 10 weeks, not 100, you know, kind of thing. And, you know, the mini example right there basically is saying.

330

00:45:24\.340 \-\-\> 00:45:43\.910

Chad Dorsey: Well, for lots of these scenarios, you would somehow have to vectorize or embed in simulations, and that means a schema for simulations in some way. Like, there's some subset of that that would probably need to happen for lots of these things to be possible, and what do we need to… what do we need to do to do that, you know?

331

00:45:44\.030 \-\-\> 00:45:52\.049

Chad Dorsey: Just wrong enough the first time around, but not cause ourselves to have to redo the whole wheel again, something like that, right?

332

00:45:58\.000 \-\-\> 00:46:11\.230

Scott Cytacki: The other thing that I wanted to mention, something I mentioned to Leslie, was that, like, so we've been looking at lab again, and part of the reason that it's annoying to develop and work with is the whole JSON description of

333

00:46:11\.580 \-\-\> 00:46:19\.999

Scott Cytacki: The interactives and all their controls and graphs, and then the layout of that with the actual simulation pane itself.

334

00:46:20\.590 \-\-\> 00:46:33\.220

Scott Cytacki: Whereas now, you know, with various AI dev tools, it's easy for them to add controls to pages in various places and all that, and to copy, designs that are images. So it would…

335

00:46:33\.350 \-\-\> 00:46:52\.730

Scott Cytacki: it seems like it would be easy to have it rebuild all the lab stuff in modern things like React that would then be easier to develop, so the simulation wouldn't change, but AI could use the simulation the same way we do within our JSON descriptions to build the surrounding stuff around it.

336

00:46:52\.760 \-\-\> 00:46:58\.660

Scott Cytacki: So it would make… it would be possible to modernize all of LEB, but then that also…

337

00:46:59\.100 \-\-\> 00:47:12\.829

Scott Cytacki: then would apply to some of the stuff we're talking about now. If we've figured out the right prompts to tell the AI to build a simulation given a picture, then it could also have a lot of data to build other kind of simulations that it doesn't have a picture.

338

00:47:12\.830 \-\-\> 00:47:21\.550

Scott Cytacki: So that it could now generate any kind of… not any, but lots of different simulations, depending on what the student… what it thinks the student needs.

339

00:47:21\.550 \-\-\> 00:47:33\.230

Chad Dorsey: And what are the prerequisites for that piece that you just described, and how is that enabled, you know, sort of secondarily by this modernization piece?

340

00:47:34\.420 \-\-\> 00:47:35\.899

Scott Cytacki: I mean, I think that…

341

00:47:36\.140 \-\-\> 00:47:48\.330

Scott Cytacki: the modernization piece will require us constructing prompts that give it enough information to know how to understand what's in the JSON and what it's possible to do with the different models, so it….

342

00:47:48\.770 \-\-\> 00:47:59\.729

Scott Cytacki: probably means we actually have to document the properties that our models support. If we have… and so the work to do all that documentation would then also benefit the

343

00:48:00\.580 \-\-\> 00:48:04\.120

Scott Cytacki: it's generating anything, basically. I see, basically.

344

00:48:04\.120 \-\-\> 00:48:11\.690

Chad Dorsey: Yeah, basically you're forced to know enough about how the machine works to reconstruct it, and then you can create variations of that, etc.

345

00:48:12\.270 \-\-\> 00:48:19\.050

Scott Cytacki: Yeah, so we described this machine well enough to the AI, and now the AI can make new variations of it.

346

00:48:22\.550 \-\-\> 00:48:23\.309

Chad Dorsey: Yeah, I assume.

347

00:48:23\.310 \-\-\> 00:48:33\.680

Scott Cytacki: That would apply to other things besides lab, but, you know, lab is the… seems like the easiest one along those lines, where it's this generic thing that can be customized in many, many different ways.

348

00:48:35\.640 \-\-\> 00:48:40\.910

Leslie Bondaryk (she/her): Yeah, I mean, the… Lab… Lab does all the right things.

349

00:48:41\.030 \-\-\> 00:48:42\.970

Chad Dorsey: In all the wrong ways, yep.

350

00:48:42\.970 \-\-\> 00:48:45\.170

Leslie Bondaryk (she/her): In a… in a… in… not all the wrong.

351

00:48:45\.170 \-\-\> 00:48:45\.890

Chad Dorsey: No.

352

00:48:45\.890 \-\-\> 00:48:58\.529

Leslie Bondaryk (she/her): In some occasionally frustrating ways. There's not… there's not adequate separation between its modeling engine and its UI layout, right? It doesn't do the MVC thing.

353

00:48:58\.530 \-\-\> 00:49:04\.799

Chad Dorsey: Despite the separate JSON files, that doesn't actually… it's sort of a facade of separation, yeah.

354

00:49:06\.030 \-\-\> 00:49:08\.180

Leslie Bondaryk (she/her): … But…

355

00:49:08\.410 \-\-\> 00:49:17\.919

Leslie Bondaryk (she/her): On the flip side, it does exactly the thing that I always recommend when I build new systems, and the model that I have followed rigorously with Clue.

356

00:49:17\.920 \-\-\> 00:49:31\.710

Leslie Bondaryk (she/her): Which is that you need to think about authoring of the activities at the same time as you think about playing them, right? It is an unwise decision to leave, like, the creation of the things as an exercise to the reader.

357

00:49:31\.710 \-\-\> 00:49:32\.380

Chad Dorsey: Men.

358

00:49:32\.770 \-\-\> 00:49:36\.369

Leslie Bondaryk (she/her): Because all sorts of good benefits accrue

359

00:49:36\.620 \-\-\> 00:49:47\.090

Leslie Bondaryk (she/her): when you… when you consider the creation of content to be the same as the playing of content to be the same as the interacting with content, right?

360

00:49:47\.090 \-\-\> 00:49:47\.690

Chad Dorsey: those….

361

00:49:47\.690 \-\-\> 00:49:57\.379

Leslie Bondaryk (she/her): to be related concepts. And I think that is even more true for the backbone of the thing that you're asking for, right? Like, how do I make it possible

362

00:49:57\.520 \-\-\> 00:50:00\.149

Leslie Bondaryk (she/her): For a machine to play.

363

00:50:00\.150 \-\-\> 00:50:00\.950

Chad Dorsey: Right?

364

00:50:00\.950 \-\-\> 00:50:08\.710

Leslie Bondaryk (she/her): That's what… that's all we're asking for here. The machine needs to be able to play in the same sandbox as the students and the teachers.

365

00:50:09\.240 \-\-\> 00:50:24\.640

Leslie Bondaryk (she/her): If you're gonna do that, then you just need to be super careful about, like, your storage formats, your representations of things, and that, you know, that every concept that happens, be it action or result.

366

00:50:25\.040 \-\-\> 00:50:30\.429

Leslie Bondaryk (she/her): Can be codified so that the machine can grab it.

367

00:50:30\.950 \-\-\> 00:50:33\.520

Chad Dorsey: Talk about it, return it to you.

368

00:50:35\.560 \-\-\> 00:50:36\.410

Leslie Bondaryk (she/her): Right? Yes.

369

00:50:37\.540 \-\-\> 00:50:48\.180

Leslie Bondaryk (she/her): But… but to do a good job on this problem, you know, I mean, we need to start talking yesterday about…

370

00:50:48\.680 \-\-\> 00:50:54\.760

Leslie Bondaryk (she/her): not… not… what does this sim look like? Right?

371

00:50:54\.760 \-\-\> 00:50:55\.149

Chad Dorsey: Which is….

372

00:50:55\.150 \-\-\> 00:50:59\.310

Leslie Bondaryk (she/her): always where the PIs want to start the conversation with me.

373

00:51:00\.090 \-\-\> 00:51:11\.589

Leslie Bondaryk (she/her): and more about what happens in that classroom, what happens on that student journey, and how do I recognize what those teachable moments are.

374

00:51:11\.590 \-\-\> 00:51:12\.210

Chad Dorsey: And annoying.

375

00:51:12\.310 \-\-\> 00:51:13\.220

Leslie Bondaryk (she/her): port them.

376

00:51:16\.700 \-\-\> 00:51:17\.510

Chad Dorsey: Yep.

377

00:51:19\.090 \-\-\> 00:51:34\.750

Leslie Bondaryk (she/her): You know, and at that point, you can make choices about whether or not you want to do the thing that chatbots do sometimes, which is, do you want to ask me about shipping? Or, you know, ordering, or something else?

378

00:51:34\.750 \-\-\> 00:51:35\.770

Chad Dorsey: Right?

379

00:51:36\.070 \-\-\> 00:51:51\.099

Leslie Bondaryk (she/her): you can put that layer on it, or you can leave it open\-ended if we decide that that's educationally the best thing, or we can put buttons on it and not show them the LLM at all, right? You know, like, those things will come.

380

00:51:51\.460 \-\-\> 00:51:55\.850

Leslie Bondaryk (she/her): But… but they're not… they're… they're an end. They're not.

381

00:51:55\.850 \-\-\> 00:51:58\.390

Chad Dorsey: the beginning. Yep.

382

00:52:01\.300 \-\-\> 00:52:03\.040

Chad Dorsey: Yeah, I agree on that.

383

00:52:05\.950 \-\-\> 00:52:10\.499

Leslie Bondaryk (she/her): And in fact, my whole beef with… with DeVai was…

384

00:52:11\.160 \-\-\> 00:52:25\.860

Leslie Bondaryk (she/her): you know, what I wrote about in that LinkedIn article. We've spent 50 years trying to figure out how to make it obvious what applications do based on their UI affordances, and with LLMs, we've thrown it all out the window.

385

00:52:26\.720 \-\-\> 00:52:28\.090

Chad Dorsey: Right.

386

00:52:28\.580 \-\-\> 00:52:29\.230

Leslie Bondaryk (she/her): Right?

387

00:52:29\.770 \-\-\> 00:52:34\.310

Leslie Bondaryk (she/her): Like, I don't actually think it's great that I can walk up to

388

00:52:35\.140 \-\-\> 00:52:43\.990

Leslie Bondaryk (she/her): a thing that is supposed to help me learn physics, and to ask it questions about English literature. Like, I think that's a problem.

389

00:52:44\.900 \-\-\> 00:52:45\.460

Leslie Bondaryk (she/her): And…

390

00:52:45\.720 \-\-\> 00:52:50\.389

Leslie Bondaryk (she/her): You know, because now I don't know what to do, you know, now I don't know where to start.

391

00:52:52\.880 \-\-\> 00:53:02\.889

Leslie Bondaryk (she/her): So if I'm learning about data science, I really feel like I ought to have some affordances that say, here, this is the important stuff, right? Focus here, right? Right, right.

392

00:53:02\.890 \-\-\> 00:53:10\.730

Chad Dorsey: Exactly, which, as to your point, we've done in everything else, whether it's bank teller windows, or UI buttons, or what have you, yeah.

393

00:53:11\.770 \-\-\> 00:53:12\.510

Leslie Bondaryk (she/her): Right, so.

394

00:53:12\.510 \-\-\> 00:53:27\.219

Scott Cytacki: And I think part of that is why it does… I mean, that's why it asks you, do you want to ask me about this? Do you want me to make a plot for this? Yeah. You know, because it's recognizing, or they're… the people training it are recognizing that that's important for it to provide you some guidance.

395

00:53:27\.220 \-\-\> 00:53:30\.209

Leslie Bondaryk (she/her): Right, which I had to fight for.

396

00:53:34\.520 \-\-\> 00:53:43\.109

Leslie Bondaryk (she/her): Right? I, I, you know, I don't want anybody to even have to ask. I want it to tell me, and I think it does… I think maybe it does now.

397

00:53:43\.310 \-\-\> 00:53:49\.409

Leslie Bondaryk (she/her): But again, this was a 6\-month long fight. Here are the attributes in this document that you're looking at.

398

00:53:49\.410 \-\-\> 00:53:50\.139

Chad Dorsey: Hmm, right.

399

00:53:50\.140 \-\-\> 00:53:52\.459

Leslie Bondaryk (she/her): like to do next.

400

00:53:53\.340 \-\-\> 00:53:54\.540

Chad Dorsey: Right, right.

401

00:53:54\.540 \-\-\> 00:54:03\.200

Leslie Bondaryk (she/her): I'm working in data science, that's what I'm doing. If I'm a student who is cited, the first thing I see when I open that document is a table.

402

00:54:03\.200 \-\-\> 00:54:07\.269

Chad Dorsey: Right, yeah, you don't see a gray screen that shows a…

403

00:54:08\.600 \-\-\> 00:54:15\.149

Chad Dorsey: magic window right around your mouse, so you have to explore the whole screen to figure out what's there, right?

404

00:54:17\.150 \-\-\> 00:54:17\.640

Leslie Bondaryk (she/her): Quit.

405

00:54:17\.640 \-\-\> 00:54:33\.339

Scott Cytacki: You know, at the same time, you know, now that we have these experts that we can ask questions of, whatever kind of questions you want, it is some skill to figure out how to keep yourself on track to learn what you originally started out wanting to learn.

406

00:54:33\.670 \-\-\> 00:54:34\.540

Scott Cytacki: …

407

00:54:35\.040 \-\-\> 00:54:43\.249

Scott Cytacki: So, there is some, I think, some new skill there that would be good for us to include in some of our materials.

408

00:54:43\.520 \-\-\> 00:55:05\.019

Leslie Bondaryk (she/her): I agree that learning self\-regulation in the presence of an open\-ended environment is a good thing to learn. I also know what a class full of 13\-year\-olds looks like as they approach a web browser that doesn't prevent them from opening other tabs and looking at videos of their favorite pop star, right?

409

00:55:05\.020 \-\-\> 00:55:17\.770

Leslie Bondaryk (she/her): focus is an issue, but I'm not sure that that's our problem to solve. Whereas affordances, I feel like, are mine, right? Like, that's my problem to help them with.

410

00:55:22\.910 \-\-\> 00:55:24\.939

Chad Dorsey: Yeah, no, I think that's fair.

411

00:55:27\.290 \-\-\> 00:55:28\.670

Chad Dorsey: Okay.

412

00:55:28\.670 \-\-\> 00:55:33\.279

Leslie Bondaryk (she/her): It's like Don Norman's… it's like Don Norman's door handles, right?

413

00:55:33\.280 \-\-\> 00:55:38\.890

Chad Dorsey: Yes. On it, or do I pull it? Yes, I totally, yeah, totally agree.

414

00:55:39\.790 \-\-\> 00:55:56\.650

Chad Dorsey: Well, this is useful, for me, and hopefully my main goal is just to get to spark these thoughts in the back of our heads for the moment, but, you know, as people are starting to think about projects, you know, and proposals, too, to some degree, but, you know, especially if we're on the ground.

415

00:55:56\.850 \-\-\> 00:55:58\.860

Chad Dorsey: Creating things, you know.

416

00:55:58\.990 \-\-\> 00:56:08\.609

Chad Dorsey: knowing what kinds of layers are, you know, important, or what kinds of considerations are key, I think, is, you know, above all. And…

417

00:56:08\.890 \-\-\> 00:56:25\.859

Chad Dorsey: And it may be that some of these things are experiments that we can try, you know, earlier on as well, in some way, shape, or form, whether it's the lab piece, which, yeah, I've, you know, thought on my runs about modernizing lab more than one hand of times.

418

00:56:25\.990 \-\-\> 00:56:27\.490

Chad Dorsey: Or…

419

00:56:27\.820 \-\-\> 00:56:36\.950

Chad Dorsey: is there an experiment that we could run, you know, for X, Y, or Z that would at least tell us something interesting, or show us something interesting? You know, and I think there is…

420

00:56:37\.450 \-\-\> 00:56:41\.780

Chad Dorsey: And maybe that's the other layer of this, like…

421

00:56:42\.440 \-\-\> 00:56:59\.529

Chad Dorsey: there's value… significant value in getting ahead of the skis of PIs who aren't going to think about the thing, the Cori problem, until they want it and then they expected it was there all along. There's also benefit in…

422

00:56:59\.530 \-\-\> 00:57:14\.690

Chad Dorsey: you know, the… the example that Wizard of Oz or not, you know, helped people sort of dream in the right direction, and it may be that there's some combination of both. You know, the one that I'm still working on, just because I haven't gotten around to making the video, but is the

423

00:57:14\.840 \-\-\> 00:57:20\.560

Chad Dorsey: You know, I… a month or two ago, I hacked together a little

424

00:57:20\.810 \-\-\> 00:57:23\.960

Chad Dorsey: plug\-in for a code app that

425

00:57:24\.000 \-\-\> 00:57:46\.540

Chad Dorsey: relays to a relay server, and then down to an MCP instance, so that you can chat with Claude and ask it, that you can connect to the number instance of that and chat with Claude, and then realized as I was doing that, that nothing would stop me from having, you know, 4 of those going at the same time with Sage models in different states of

426

00:57:46\.540 \-\-\> 00:58:11\.449

Chad Dorsey: trying to solve a problem, and, you know, very easily say, you know, tell me what the class is doing, and, you know, have it go out as a little agent and check on the states of each of those, and since it's got full access to the Sage Modeler API that, you know, I created, to say, you know, give a little help to student X, you know, give them a node that they're missing, or something like that, right? And so, are there, you know, hacked\-together examples of things that might make people go.

427

00:58:11\.450 \-\-\> 00:58:25\.039

Chad Dorsey: oh, so that's a way that, you know, some interaction could happen, or some model for thinking might exist that we wouldn't have, you know, I wouldn't have uncovered otherwise. So as we're digging in the

428

00:58:25\.420 \-\-\> 00:58:36\.639

Chad Dorsey: in our development, in the back of our minds, maybe that we just want to make space for some of those things as well, because those are really generative for people, and they're not going to come up with them.

429

00:58:40\.320 \-\-\> 00:58:42\.770

Chad Dorsey: Cool. Well, thanks for indulging me.

430

00:58:43\.210 \-\-\> 00:58:49\.099

Chad Dorsey: Hopefully that gives you something to think about on your flight to Europe as well, and then you can forget about it instantly, which is great.

431

00:58:51\.970 \-\-\> 00:58:54\.180

Chad Dorsey: And…

432

00:58:54\.180 \-\-\> 00:59:12\.210

Chad Dorsey: Is there anything we need to talk about for the… for Lucas in the last 7 minutes? I mean, I think we're pretty much on the same page in Slack about, you know, both state of model and conversation stakes. I'll… I can be at part of the meeting, I don't need to be, I can only be at the first part if I am.

433

00:59:12\.680 \-\-\> 00:59:18\.360

Chad Dorsey: Yeah, it's fine. Like, again, great if you show up. Mostly, I want Scott and… Yeah.

434

00:59:18\.420 \-\-\> 00:59:32\.310

Leslie Bondaryk (she/her): Scott and Joel to know each other, because Joel will now try and pick up whatever you build for him, and build it into his thing, something will go wrong, he'll need to ask questions. That's not a me problem, that's a Scott problem, right?

435

00:59:32\.310 \-\-\> 00:59:33\.100

Chad Dorsey: Yep.

436

00:59:33\.970 \-\-\> 00:59:46\.850

Leslie Bondaryk (she/her): I'll make the connection, figure out whether the next steps that they're asking for sound like they're within scope or insane. And, you know, and then I'll… then I'm off, right? So….

437

00:59:46\.850 \-\-\> 00:59:48\.050

Chad Dorsey: Exactly.

438

00:59:49\.810 \-\-\> 00:59:50\.360

Leslie Bondaryk (she/her): Oh, no.

439

00:59:50\.360 \-\-\> 00:59:50\.950

Scott Cytacki: It sounded like….

440

00:59:50\.950 \-\-\> 00:59:51\.349

Chad Dorsey: Like, it's….

441

00:59:51\.350 \-\-\> 01:00:08\.649

Scott Cytacki: you know, to tell them that we have features that we don't support, it doesn't support, so that we would need funding in order to add those, in order to build it. Is it also fine to say that we haven't really developed this thing for more than 10 years, and that in order to add one new feature, we have to get over this 10\-year hump of.

442

01:00:08\.650 \-\-\> 01:00:13\.810

Leslie Bondaryk (she/her): I wouldn't… I wouldn't go into… like, I don't need to get into that, right?

443

01:00:13\.810 \-\-\> 01:00:14\.610

Chad Dorsey: Yeah.

444

01:00:15\.380 \-\-\> 01:00:15\.940

Leslie Bondaryk (she/her): And…

445

01:00:15\.940 \-\-\> 01:00:40\.469

Leslie Bondaryk (she/her): They did give us a little pot of money so that we could give them a prototype, right? That's what we all agreed. And in fact, Joel was even last time trying to, like, reduce the scope of the physics that he was asking for, which I, you know, I was like, oh, you're doing me a solid, isn't that nice, right? You know, trying to make it… make my life easier. But at the end of the day, I feel like we need to give him a thing

446

01:00:40\.470 \-\-\> 01:00:42\.510

Leslie Bondaryk (she/her): That does the nominal.

447

01:00:42\.850 \-\-\> 01:00:43\.470

Chad Dorsey: Yeah.

448

01:00:43\.610 \-\-\> 01:00:55\.290

Leslie Bondaryk (she/her): physics here, even if you have to fake it, and that's okay, right? Like, you have to, you know, put a… put a shim in here so that the temperature in the room goes up.

449

01:00:55\.610 \-\-\> 01:01:10\.950

Chad Dorsey: Oh, to be clear, like, there is not a… there's not a useful MW model or activity out there that doesn't have 10 of those in lots of different ways. Invisible walls, and some things, and, you know, caps on temperature, and, like, that's just the way it works with this something.

450

01:01:10\.950 \-\-\> 01:01:26\.659

Leslie Bondaryk (she/her): So if you feel like you've spent, you know, you've spent your time… you've spent your sim development time, and it's clear that you have, right? It looks like it's supposed to. It kind of behaves like it's supposed to. The only problem is that the internal

451

01:01:27\.040 \-\-\> 01:01:29\.580

Leslie Bondaryk (she/her): Temperature of the room doesn't go up.

452

01:01:30\.530 \-\-\> 01:01:40\.400

Leslie Bondaryk (she/her): when it's glass, and it must, or it doesn't show anything, right? It doesn't show anything useful at that point. So, I don't know, fake it.

453

01:01:40\.400 \-\-\> 01:01:40\.780

Chad Dorsey: If you….

454

01:01:40\.780 \-\-\> 01:01:53\.750

Leslie Bondaryk (she/her): If you've… or if you feel like it's a half a day to add the, you know, the thermal thing, or the walls, or however you want to make it happen, then do that. But I, you know, like, I wouldn't spend another week on modeling.

455

01:01:54\.500 \-\-\> 01:02:03\.720

Scott Cytacki: Yeah, no, I wouldn't… I think it's a half day, I'll have it done by tomorrow, but then there'll be, like, an inside surface that can heat up, and that's more…

456

01:02:03\.960 \-\-\> 01:02:07\.140

Scott Cytacki: Accurate to what would… the way things work in real life.

457

01:02:08\.350 \-\-\> 01:02:10\.000

Chad Dorsey: Yeah, I agree, that's totally fine.

458

01:02:11\.290 \-\-\> 01:02:19\.029

Leslie Bondaryk (she/her): Right. But, you know, but the fact that they have now paid for us to migrate over to some version of GitHub Actions is huge, right?

459

01:02:19\.030 \-\-\> 01:02:24\.780

Chad Dorsey: Exactly, yeah, we can actually think about this, you know, we can use lab in a sentence, that's good.

460

01:02:25\.040 \-\-\> 01:02:39\.509

Scott Cytacki: Yeah, I mean, that was… it's still kind of a hack, so it's good enough for this, but it's at least a step in the right direction. And… but yeah. But yeah, so I won't… I won't bring that up. But yeah, I don't… I mean, that's sort of an unknown, like, what that… getting over that hump would cost us, you know?

461

01:02:39\.510 \-\-\> 01:02:40\.030

Chad Dorsey: Right.

462

01:02:40\.250 \-\-\> 01:02:46\.259

Scott Cytacki: I don't know if it's weeks or months to be able to add, say, that thermal radiation feature.

463

01:02:47\.700 \-\-\> 01:03:02\.579

Leslie Bondaryk (she/her): Yeah, so we'll see. We'll see what they want next. You know, my fear is that… that they suddenly go, you know what, this is… this was insane, we don't really want this, we want something totally other, right? I'm trying to balance….

464

01:03:02\.580 \-\-\> 01:03:19\.169

Chad Dorsey: Yeah, I mean, we're trying to give them enough things that are clear and whatever. I mean, they don't know what they want, they know the neighborhood they're walking in, and, you know, we want to give them houses that look like they're in that neighborhood, mostly. And right now, they're barely even at that stage. They really want something that they can…

465

01:03:19\.170 \-\-\> 01:03:27\.549

Chad Dorsey: Put in a mock\-up, and sort of verify that when you press the button outside the thing, it's gonna still show it and work.

466

01:03:30\.010 \-\-\> 01:03:30\.700

Chad Dorsey: So, yeah.

467

01:03:32\.480 \-\-\> 01:03:33\.670

Chad Dorsey: But… Alright.

468

01:03:33\.670 \-\-\> 01:03:40\.540

Leslie Bondaryk (she/her): If nothing else, I've just paid for a week for Scott to, like, familiarize himself with what this code looks like.

469

01:03:40\.540 \-\-\> 01:03:58\.900

Chad Dorsey: Exactly, which I think is huge, and gets us to the point of all saying, okay, and could we, you know, plug it into, you know, whatever, cursor LLM could claw data, you know, and modernize it, you know, someday, if the answer is no, we've also learned something, that's great.

470

01:04:01\.120 \-\-\> 01:04:01\.770

Chad Dorsey: Cool.

471

01:04:02\.540 \-\-\> 01:04:14\.990

Chad Dorsey: Great. Well, thanks a lot. I'll try to pop in for the first few minutes of this conversation, which is undoubtedly going to be about new proposals that can or can't exist, Leslie, and then I have to leave, but….

472

01:04:14\.990 \-\-\> 01:04:17\.500

Leslie Bondaryk (she/her): Please do. I don't know what to tell AJ.

473

01:04:17\.500 \-\-\> 01:04:26\.810

Chad Dorsey: Very good, exactly. All right, thanks, Scott. I'll be around, even though Leslie won't, so I'll be on Slack for questions about this model and stuff in the next weeks.

474

01:04:27\.510 \-\-\> 01:04:28\.030

Scott Cytacki: Third.

475

01:04:28\.550 \-\-\> 01:04:29\.600

Scott Cytacki: Take care, bye.


