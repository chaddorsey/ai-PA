version: "3.9"

services:
  # --- Database Layer ---
  supabase-db:
    image: supabase/postgres:15.8.1.060
    container_name: supabase-db
    restart: unless-stopped
#    ports:
#      - "5432:5432"
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - supabase_db:/var/lib/postgresql/data
    networks: [pa-internal]
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "service=supabase-db"
      - "component=database"
      - "network=pa-internal"

  # --- Supabase Services ---
  supabase-rest:
    image: postgrest/postgrest:v12.2.12
    depends_on: 
      supabase-db:
        condition: service_healthy
    environment:
      PGRST_DB_URI: postgres://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/postgres
      PGRST_DB_ANON_ROLE: anon
    networks: [pa-internal]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  supabase-auth:
    image: supabase/gotrue:v2.177.0
    depends_on:
      supabase-db:
        condition: service_healthy
    environment:
      GOTRUE_DB_DATABASE_URL: postgres://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/postgres
    networks: [pa-internal]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9999/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  supabase-realtime:
    image: supabase/realtime:v2.34.47
    depends_on:
      supabase-db:
        condition: service_healthy
    networks: [pa-internal]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  supabase-meta:
    image: supabase/postgres-meta:v0.91.0
    container_name: supabase-meta
    restart: unless-stopped
    depends_on:
      supabase-db:
        condition: service_healthy
    environment:
      PG_META_PORT: 8080
      PG_META_DB_HOST: supabase-db
      PG_META_DB_PORT: 5432
      PG_META_DB_NAME: postgres
      PG_META_DB_USER: postgres
      PG_META_DB_PASSWORD: ${POSTGRES_PASSWORD}
    networks: [pa-internal]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  supabase-studio:
    image: supabase/studio:2025.06.30-sha-6f5982d
    container_name: supabase-studio
    restart: unless-stopped
    depends_on:
      supabase-db:
        condition: service_healthy
      supabase-meta:
        condition: service_healthy
      supabase-auth:
        condition: service_healthy
      supabase-rest:
        condition: service_healthy
    environment:
      # URL Studio uses to connect to Postgres directly
      SUPABASE_DB_URL: postgres://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/postgres

      # Where Studio finds Postgres Meta (schema/foreign key info)
      STUDIO_PG_META_URL: http://supabase-meta:8080

      # RESTful entrypoint via Kong gateway
      SUPABASE_URL: http://supabase-kong:8000

      # Keys (must be in your .env)
      SUPABASE_ANON_KEY: ${SUPABASE_ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SUPABASE_SERVICE_KEY}

      # Pass Postgres password explicitly as well
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    networks: [pa-internal]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # --- Workflow Automation (n8n) ---
  n8n:
    image: docker.n8n.io/n8nio/n8n
    container_name: n8n
    restart: unless-stopped
    networks: [pa-internal]
    depends_on:
      supabase-db:
        condition: service_healthy
    ports:
      - "5678:5678"  # Expose n8n web interface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5678/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    environment:
      DB_TYPE: postgresdb
      DB_POSTGRESDB_DATABASE: n8n_restore   # ðŸ‘ˆ updated here
      DB_POSTGRESDB_HOST: supabase-db
      DB_POSTGRESDB_PORT: 5432
      DB_POSTGRESDB_USER: postgres
      DB_POSTGRESDB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_POSTGRESDB_SCHEMA: public
      GENERIC_TIMEZONE: "America/New_York"
      TZ: "America/New_York"
      N8N_ENCRYPTION_KEY: ${N8N_ENCRYPTION_KEY}
      N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: "true"
      N8N_WEBHOOK_URL: ${N8N_WEBHOOK_URL}
      WEBHOOK_URL: ${WEBHOOK_URL} 
      N8N_PROTOCOL: "https"
      N8N_HOST: "0.0.0.0"
      N8N_PORT: "5678"
      N8N_EDITOR_BASE_URL: "https://n8n.cd-ai-pa.work"
    volumes:
      - n8n_data:/home/node/.n8n
      - /tmp/granola-to-letta:/tmp/granola-notes


  # --- Neo4j for Graphiti ---
  neo4j:
    image: neo4j:5.26
    container_name: graphiti-neo4j
    restart: unless-stopped
    networks: [pa-internal]
    environment:
      - NEO4J_AUTH=neo4j/demodemo
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7474/browser/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # --- Graphiti MCP Server ---
  graphiti-mcp-server:
    build:
      context: ./graphiti/mcp_server
    container_name: graphiti-mcp-server
    user: root
    restart: unless-stopped
    networks: [pa-internal]
    depends_on:
      neo4j:
        condition: service_healthy
    working_dir: /app
    entrypoint: ["/root/.local/bin/uv","run","graphiti_mcp_server.py"]
    environment:
      # Common MCP variables
      - MCP_SERVER_NAME=graphiti-tools
      - MCP_SERVER_VERSION=1.0.0
      - MCP_SERVER_DESCRIPTION=Graphiti memory and knowledge graph tools
      - MCP_SERVER_HOST=0.0.0.0
      - MCP_SERVER_PORT=8082
      - MCP_SERVER_PATH=/mcp
      - MCP_LOG_LEVEL=INFO
      - MCP_LOG_FORMAT=json
      - MCP_HEALTH_CHECK_PATH=/health
      
      # Graphiti-specific variables
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - MODEL_NAME=${MODEL_NAME:-gpt-4.1-mini}
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=demodemo
      - GRAPHITI_TELEMETRY_ENABLED=false
      - SEMAPHORE_LIMIT=10
      - RESET_NEO4J=true
      - HOST=0.0.0.0
      - PORT=8082
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 45s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service,component,network"
    labels:
      - "service=graphiti-mcp-server"
      - "component=mcp-server"
      - "network=pa-internal"
      - "mcp-transport=http"
      - "mcp-version=1.0.0"
      - "security.internal-only=true"
      - "security.no-external-access=true"

  # --- RAG MCP Server ---
  rag-mcp-server:
    build:
      context: ./rag-mcp
      dockerfile: Dockerfile
    container_name: rag-mcp-server
    restart: unless-stopped
    networks: [pa-internal]
    volumes:
      - rag-mcp-data:/app/data
    environment:
      # Common MCP variables
      - MCP_SERVER_NAME=rag-tools
      - MCP_SERVER_VERSION=1.0.0
      - MCP_SERVER_DESCRIPTION=RAG tools for retrieval-augmented generation
      - MCP_SERVER_HOST=0.0.0.0
      - MCP_SERVER_PORT=8082
      - MCP_SERVER_PATH=/mcp
      - MCP_LOG_LEVEL=INFO
      - MCP_LOG_FORMAT=json
      - MCP_HEALTH_CHECK_PATH=/health
      
      # RAG-specific variables (placeholders for future implementation)
      - VECTOR_DB_URL=http://vector-db:8080
      - DOCUMENT_STORE_URL=http://document-store:8080
      - EMBEDDING_MODEL=text-embedding-ada-002
      - PORT=8082
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service,component,network"
    labels:
      - "service=rag-mcp-server"
      - "component=mcp-server"
      - "network=pa-internal"
      - "mcp-transport=http"
      - "mcp-version=1.0.0"
      - "security.internal-only=true"
      - "security.no-external-access=true"

  # --- Health Monitor Service ---
  health-monitor:
    build:
      context: ./health-monitor
      dockerfile: Dockerfile
    container_name: health-monitor
    restart: unless-stopped
    networks: [pa-internal]
    depends_on:
      - gmail-mcp-server
      - graphiti-mcp-server
      - rag-mcp-server
    environment:
      - NODE_ENV=production
      - PORT=8083
      - HEALTH_CHECK_INTERVAL=30
    ports:
      - "8083:8083"  # Expose for external access to dashboard
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service,component,network"
    labels:
      - "service=health-monitor"
      - "component=monitoring"
      - "network=pa-internal"
      - "external-access=true"
      - "dashboard=true"


# Cloudflare

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: cloudflared
    restart: unless-stopped
    networks: [pa-internal]
    depends_on:
      n8n:
        condition: service_healthy
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
    command: tunnel run
    healthcheck:
      test: ["CMD", "cloudflared", "tunnel", "info"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "service=cloudflared"
      - "component=tunnel"
      - "network=pa-internal"
      - "external-access=true"


  # Letta is an agent building framework with built-in memory/vectordb support.
  # https://docs.letta.com/quickstart/docker
  letta:
    image: letta/letta:latest
    ports:
      - "8283:8283"  # Letta web interface
    volumes:
      - ./letta/letta_mcp_config.json:/root/.letta/mcp_config.json
    networks: [pa-internal]
    environment:
      LETTA_DEBUG: "${LETTA_DEBUG:-false}"
      LETTA_PG_URI: "postgresql://postgres:${POSTGRES_PASSWORD}@supabase-db:5432/postgres"
      REQUESTS_CA_BUNDLE: /etc/ssl/certs/ca-certificates.crt
      # https://docs.letta.com/guides/server/providers/anthropic
      ANTHROPIC_API_KEY: $ANTHROPIC_API_KEY
      # https://docs.letta.com/guides/server/providers/google
      GEMINI_API_KEY: $GEMINI_API_KEY
      #OLLAMA_BASE_URL: $OLLAMA_BASE_URL
      OPENAI_API_KEY: $OPENAI_API_KEY

      # Do not attempt to use Ollama, LM Studio or OpenRouter chat models with Letta.
      # Letta is *very* picky and will only reliably work with the major cloud providers.
      # Embedding models will work fine, but only uncomment if you know what you're doing.
    restart: on-failure
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8283/v1/health/"]
      interval: 5s
      timeout: 5s
      retries: 18
      start_period: 1s


  # Open WebUI is the front-end UI to Letta
  open-webui:
    image: ghcr.io/open-webui/open-webui:0.6.22
    container_name: open-webui
    volumes:
     - open-webui:/app/backend/data
    networks: [pa-internal]
    environment:
      # https://docs.openwebui.com/getting-started/env-configuration/
      - GLOBAL_LOG_LEVEL=${OPENWEBUI_LOG_LEVEL:-INFO}
      # Disable admin login
      - WEBUI_AUTH=false
      # Enable the /docs endpoint for OpenAPI viewing
      #- ENV=dev
      # Prevent a langchain warning
      - USER_AGENT=openwebui
      #Â Set tags and titles explictly
      - ENABLE_TAGS_GENERATION=$ENABLE_TAGS_GENERATION
      - ENABLE_TITLE_GENERATION=$ENABLE_TITLE_GENERATION
      - TASK_MODEL=$TASK_MODEL
      - TASK_MODEL_EXTERNAL=$TASK_MODEL_EXTERNAL
      # Disable some meaningless options
      - ENABLE_EVALUATION_ARENA_MODELS=false
      - ENABLE_AUTOCOMPLETE_GENERATION=false
      - ENABLE_RETRIEVAL_QUERY_GENERATION=false
      - ENABLE_FOLLOW_UP_GENERATION=false
      # OpenAI selection should go to Hayhooks to show agents
      - ENABLE_OPENAI_API=true
      - OPENAI_API_BASE_URL=http://hayhooks:1416
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Ollama Options
      - ENABLE_OLLAMA_API=$ENABLE_OLLAMA_API
      - OLLAMA_BASE_URL=$OLLAMA_BASE_URL
      # RAG options can be transformers, ollama, or openai 
      - RAG_EMBEDDING_ENGINE=$RAG_EMBEDDING_ENGINE
      - RAG_EMBEDDING_MODEL=$RAG_EMBEDDING_MODEL
      - RAG_OPENAI_API_BASE_URL=http://litellm:4000
      - RAG_OPENAI_API_KEY=$LITELLM_MASTER_KEY
      - RAG_OLLAMA_BASE_URL=$OLLAMA_BASE_URL
      # Tavily Web Search in Open WebUI
      - ENABLE_WEB_SEARCH=$ENABLE_WEB_SEARCH
      - WEB_SEARCH_ENGINE=$WEB_SEARCH_ENGINE
      - TAVILY_API_KEY=$TAVILY_API_KEY
      # Audio options
      - AUDIO_STT_ENGINE=$AUDIO_STT_ENGINE
    restart: unless-stopped
    # https://docs.openwebui.com/getting-started/advanced-topics/monitoring/#basic-health-check-endpoint
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 18
      start_period: 5s


  slack-mcp-server:
    image: ghcr.io/korotovsky/slack-mcp-server:latest
    restart: unless-stopped
    networks: [pa-internal]
    volumes:
      - users_cache:/app/mcp-server/.users_cache.json
      - channels_cache:/app/mcp-server/.channels_cache.json
    env_file:
      - .env
    environment:
      SLACK_MCP_HOST: "0.0.0.0"
      SLACK_MCP_PORT: "3001"
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "3001"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s

  # --- Slackbot Service ---
  slackbot:
    build:
      context: ./slackbot
      dockerfile: Dockerfile
    container_name: slackbot
    restart: unless-stopped
    networks: [pa-internal]
    volumes:
      - slackbot_state:/app/state_store
      - slackbot_logs:/app/logs
    env_file:
      - .env
    environment:
      SLACK_BOT_TOKEN: ${SLACK_BOT_TOKEN}
      SLACK_APP_TOKEN: ${SLACK_APP_TOKEN}
      LETTA_BASE_URL: "http://letta:8283"
      LETTA_AGENT_ID: ${LETTA_AGENT_ID}
      HEALTH_CHECK_PORT: "8081"
    ports:
      - "8081:8081"  # Health check port
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "service=slackbot"
      - "component=chatbot"
      - "network=pa-internal"

  # --- Cloudflare Tunnel Service ---
  cloudflare-tunnel:
    image: cloudflare/cloudflared:latest
    container_name: cloudflare-tunnel
    restart: unless-stopped
    networks: [pa-internal]
    volumes:
      - /Users/dorseyhomeserver/.cloudflared:/home/nonroot/.cloudflared:ro
    command: tunnel --no-autoupdate --config /home/nonroot/.cloudflared/config.yml run pa-server
    depends_on:
      - letta
      - open-webui
      - n8n
      - slackbot
    healthcheck:
              test: ["CMD", "cloudflared", "tunnel", "info", "pa-ecosystem-tunnel"]
              interval: 30s
              timeout: 10s
              retries: 3
              start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      - "service=cloudflare-tunnel"
      - "component=external-access"
      - "network=pa-internal"

  gmail-mcp-server:
    build:
      context: ./gmail-mcp
      dockerfile: Dockerfile
    container_name: gmail-mcp-server
    restart: unless-stopped
    networks: [pa-internal]
    volumes:
      - gmail-mcp-data:/app/data
      - ./gmail-mcp/gcp-oauth.keys.json:/app/config/gcp-oauth.keys.json:ro
    environment:
      # Common MCP variables
      - MCP_SERVER_NAME=gmail-tools
      - MCP_SERVER_VERSION=1.1.11
      - MCP_SERVER_DESCRIPTION=Gmail integration tools
      - MCP_SERVER_HOST=0.0.0.0
      - MCP_SERVER_PORT=8080
      - MCP_SERVER_PATH=/mcp
      - MCP_LOG_LEVEL=INFO
      - MCP_LOG_FORMAT=json
      - MCP_HEALTH_CHECK_PATH=/health
      
      # Gmail-specific variables
      - GMAIL_OAUTH_PATH=/app/config/gcp-oauth.keys.json
      - GMAIL_CREDENTIALS_PATH=/app/data/credentials.json
      - PORT=8080
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service,component,network"
    labels:
      - "service=gmail-mcp-server"
      - "component=mcp-server"
      - "network=pa-internal"
      - "mcp-transport=http"
      - "mcp-version=1.1.11"
      - "security.internal-only=true"
      - "security.no-external-access=true"

volumes:
  supabase_db:
    external: true
    name: ai-pa_supabase_db
  n8n_data:
    name: ai-pa_n8n_data
  neo4j_data:
  neo4j_logs:
  open-webui:  
  users_cache:
  channels_cache:
  slackbot_state:
  slackbot_logs:
  gmail-mcp-data:
  rag-mcp-data:

# --- Monitoring and Logging Configuration ---
# All services use standardized logging and labeling for unified monitoring

networks:
  pa-internal:
    name: pa-internal
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "true"
      com.docker.network.driver.mtu: "1500"
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
    labels:
      - "project=pa-ecosystem"
      - "environment=production"
      - "security=internal-only"

# Standard logging configuration for all services:
# logging:
#   driver: "json-file"
#   options:
#     max-size: "10m"
#     max-file: "3"
#
# Standard labeling for all services:
# labels:
#   - "service=<service-name>"
#   - "component=<component-type>"
#   - "network=pa-internal"
